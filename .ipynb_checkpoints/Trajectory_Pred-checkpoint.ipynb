{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/akanu/git/deep_sort_yolov3/Bounding_box'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akanu/git/deep_sort_yolov3/Bounding_box\n"
     ]
    }
   ],
   "source": [
    "cd '/home/akanu/git/deep_sort_yolov3/Bounding_box'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "# from tensorflow.python.ops import math_ops\n",
    "import tensorflow.keras.backend as kb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 5, 1, 2, 4, 8, 0, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.random.permutation(len([1,4,5,6,6,7,8,88,8]))\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Files_Load():\n",
    "    train_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Train_Box/\"\n",
    "    test_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Test_Box/\"\n",
    "    box_train_txt = os.listdir(train_file)\n",
    "    box_train_txt.sort()\n",
    "    box_test_txt = os.listdir(test_file)\n",
    "    box_test_txt.sort()\n",
    "    \n",
    "    loc_files_train, loc_files_test = [], []\n",
    "    \n",
    "    for txt in box_train_txt:\n",
    "        loc_files_train.append(train_file + txt)\n",
    "    for txt in box_test_txt:\n",
    "        loc_files_test.append(test_file + txt)\n",
    "    \n",
    "    return loc_files_train, loc_files_test, box_train_txt, box_test_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Boxes(loc_files, txt_names, time_steps, pad ='pre'):\n",
    "    \"\"\"\n",
    "    loc_files: List that contains that has text files save\n",
    "    txt_names: Txt file names. For visualization process\n",
    "    time_step: Sequence length input\n",
    "    pad: inputs 'pre' or 'post'\n",
    "    \n",
    "    x_person_box: Has bounding box locations\n",
    "    y_person_box: Label for bounding box locations\n",
    "    frame_person_id: Contains frame Number and person_Id of entire sequence, \n",
    "                     Last element is prediction frame. For visulization process\n",
    "    video_file: Points to video file used. For visulization process\n",
    "    \"\"\"\n",
    "    \n",
    "    x_ppl_box, y_ppl_box, frame_ppl_id, video_file, abnormal = [], [], [], [],[]  #Has bounding box locations inside\n",
    "    \n",
    "    #For splitting process\n",
    "    split_train_test = 0\n",
    "    split = 0\n",
    "    find_split = 0\n",
    "    \n",
    "    # Tells me how many in sequence was short.\n",
    "    # Do I want to go back and count for train and test seperatly \n",
    "    short_len = 0\n",
    "    \n",
    "#     datadict = OrderedDict()\n",
    "    datadict = {}\n",
    "    \n",
    "    for loc, txt_name in zip(loc_files, txt_names):\n",
    "        data = pd.read_csv(loc, ' ' )\n",
    "        # Note that person_box is 1 behind ID\n",
    "        max_person = data['Person_ID'].max()\n",
    "        for num in range(1,max_person+1):\n",
    "            temp_box = data[data['Person_ID'] == num ]['BB_tl_0\tBB_tl_1\tBB_br_0\tBB_br_1'.split()].values\n",
    "            person_seq_len = len(temp_box)\n",
    "            temp_frame_id = data[data['Person_ID'] == num ]['Frame_Number Person_ID'.split()].values\n",
    "            abnormal_frame_ped = data[data['Person_ID'] == num]['anomaly'].values\n",
    "            if person_seq_len > time_steps:\n",
    "                for i in range(0, person_seq_len - time_steps):\n",
    "                    temp_person_box = temp_box[i:(i+time_steps)]\n",
    "                    temp_fr_person_id = temp_frame_id[i:(i+time_steps+1)]\n",
    "\n",
    "                    x_ppl_box.append(temp_person_box)\n",
    "                    y_ppl_box.append(temp_box[i+time_steps])\n",
    "                    \n",
    "                    assert temp_person_box.shape == (time_steps,4)\n",
    "                    assert temp_fr_person_id.shape  == (time_steps+1,2), print(temp_fr_person_id.shape)\n",
    "                    \n",
    "                    frame_ppl_id.append(temp_fr_person_id)\n",
    "                    \n",
    "                    video_file.append(txt_name)      \n",
    "                    abnormal.append(abnormal_frame_ped[i+time_steps]) #Finds if predicted frame is abnormal\n",
    "                    \n",
    "            elif person_seq_len == 1:\n",
    "                # want it to skip loop\n",
    "                continue\n",
    "            elif person_seq_len <= time_steps:\n",
    "                temp_person_box_unpad = temp_box\n",
    "                temp_fr_person_id_unpad = temp_frame_id\n",
    "                temp_person_box = pad_sequences(temp_person_box_unpad.T, maxlen = time_steps+1, padding = pad).T\n",
    "                temp_fr_person_id = pad_sequences(temp_fr_person_id_unpad.T,  maxlen = time_steps+1, padding = pad).T\n",
    "                \n",
    "                assert temp_person_box.shape == (time_steps+1,4)\n",
    "                assert temp_fr_person_id.shape  == (time_steps+1,2)\n",
    "                \n",
    "                x_ppl_box.append(temp_person_box[0:time_steps,:])\n",
    "                y_ppl_box.append(temp_person_box[time_steps,:])\n",
    "                \n",
    "                frame_ppl_id.append(temp_fr_person_id[0:time_steps+1,:])\n",
    "                \n",
    "                video_file.append(txt_name)\n",
    "                abnormal.append(abnormal_frame_ped[-1]) #Finds if predicted frame is abnormal\n",
    "\n",
    "            else:\n",
    "                print('error')\n",
    "\n",
    "    rand = np.random.permutation(len(x_ppl_box))\n",
    "    datadict['x_ppl_box'] = np.array(x_ppl_box)[rand]\n",
    "    datadict['y_ppl_box'] = np.array(y_ppl_box)[rand]\n",
    "    datadict['frame_ppl_id'] = np.array(frame_ppl_id)[rand]\n",
    "    datadict['video_file'] = np.array(video_file)[rand]\n",
    "    datadict['abnormal'] = np.array(abnormal)[rand]\n",
    "\n",
    "        \n",
    "    return  datadict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoPlot(y_ppl_box,y_pred, frame_ppl_id, video_file, vid_type ='test'):\n",
    "    \"\"\"\n",
    "    Function is plotting the ground truth and predicted \n",
    "    \n",
    "    y_ppl_box: Label for bounding box locations\n",
    "    y_ppl_box_pred: Prediction from Model\n",
    "    frame_person_id: Contains frame Number and person_Id of entire sequence, \n",
    "                     Last element is prediction frame. For visulization process\n",
    "    video_file: Points to video file used. For visulization process\n",
    "    \"\"\"\n",
    "    # Need a way to save images\n",
    "    file = {}\n",
    "    file['train'] = '/home/akanu/Dataset/Anomaly/Avenue_Dataset/training_videos/'\n",
    "    file['test'] = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/testing_videos/\"\n",
    "    loc_videos = file[vid_type] + video_file[0][:2] + '.' + video_file[0][2:5]\n",
    "    ###\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(loc_videos)\n",
    "    \n",
    "    for i in range(-1, frame_ppl_id[0,-1,0] + 1):\n",
    "        #Assune sequences are connected and don't skip cuz of occlusions \n",
    "        print(i)\n",
    "        ret, frame = video_capture.read()\n",
    "        if i == frame_ppl_id[0,-1,0]:\n",
    "            pred_frame = frame.copy()\n",
    "            cv2.rectangle(frame, (int(y_ppl_box[0,0]), int(y_ppl_box[0,1])), (int(y_ppl_box[0,2]), int(y_ppl_box[0,3])),(255,255,255), 2)\n",
    "            cv2.putText(frame, str(frame_ppl_id[0,-1,1]),(int(y_ppl_box[0,0]), int(y_ppl_box[0,1])),0, 5e-3 * 200, (0,255,0),2)\n",
    "\n",
    "            cv2.rectangle(pred_frame, (int(y_pred[0,0]), int(y_pred[0,1])), (int(y_pred[0,2]), int(y_pred[0,3])),(255,255,0), 2)\n",
    "            cv2.putText(pred_frame, str(frame_ppl_id[0,-1,1]),(int(y_pred[0,0]), int(y_pred[0,1])),0, 5e-3 * 200, (0,255,0),2)\n",
    "            \n",
    "            cv2.imwrite('/home/akanu/git/deep_sort_yolov3/Images_saved/pred.jpg', pred_frame)\n",
    "            cv2.imwrite(\"/home/akanu/git/deep_sort_yolov3/Images_saved/gt.jpg\", frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(y, x):\n",
    "    xA = kb.max((x[:,0:1],y[:,0:1]), axis=0,keepdims=True)\n",
    "    yA = kb.max((x[:,1:2],y[:,1:2]), axis=0,keepdims=True)\n",
    "    xB = kb.min((x[:,2:3],y[:,2:3]), axis=0,keepdims=True)\n",
    "    yB = kb.min((x[:,3:4],y[:,3:4]), axis=0,keepdims=True)\n",
    "\n",
    "    interArea1 = kb.max((kb.zeros_like(xB), (xB-xA +1) ), axis=0, keepdims=True)\n",
    "    interArea2 = kb.max((kb.zeros_like(xB), (yB-yA +1) ), axis=0, keepdims=True)\n",
    "    interArea = interArea1*interArea2\n",
    "    boxAArea = (x[:,2:3] - x[:,0:1] + 1) * (x[:,3:4] - x[:,1:2] + 1)\n",
    "    boxBArea = (y[:,2:3] - y[:,0:1] + 1) * (y[:,3:4] - y[:,1:2] + 1)\n",
    "\n",
    "    iou = interArea / (boxAArea + boxBArea - interArea)\n",
    "    iou_mean = kb.mean(iou)\n",
    "    return iou_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading with 20 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8500730991363525\n"
     ]
    }
   ],
   "source": [
    "loc_files_train, loc_files_test, box_train_txt, box_test_txt = Files_Load()\n",
    "start = time.time()\n",
    "# x_ppl_box, y_ppl_box, frame_ppl_id, video_file,trialdict = Boxes(loc_files_train, box_train_txt, 20, pad ='pre')\n",
    "traindict = Boxes(loc_files_train, box_train_txt, 20, pad ='pre')\n",
    "end = time.time()\n",
    "print(end -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['04.txt', '08.txt', '13.txt', ..., '10.txt', '04.txt', '10.txt'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindict['video_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 6, 5, 2, 0, 1, 7, 4])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = np.random.permutation(len(p))\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 10,  8,  7,  4,  1,  2,  9,  6])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(p)[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[443, 104, 484, 233],\n",
       "       [443, 104, 484, 233],\n",
       "       [443, 104, 484, 233],\n",
       "       [443, 104, 484, 233],\n",
       "       [443, 104, 484, 233],\n",
       "       [442, 104, 484, 233],\n",
       "       [442, 104, 484, 233],\n",
       "       [443, 104, 484, 233],\n",
       "       [443, 104, 484, 233],\n",
       "       [443, 104, 484, 233],\n",
       "       [443, 104, 484, 234],\n",
       "       [442, 104, 484, 234],\n",
       "       [442, 104, 484, 234],\n",
       "       [442, 105, 484, 235],\n",
       "       [442, 105, 484, 235],\n",
       "       [442, 105, 484, 235],\n",
       "       [442, 106, 484, 234],\n",
       "       [442, 106, 484, 234],\n",
       "       [442, 105, 484, 234],\n",
       "       [442, 105, 484, 234]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindict['x_ppl_box'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[443, 104, 484, 233],\n",
       "        [443, 104, 484, 233],\n",
       "        [443, 104, 484, 233],\n",
       "        ...,\n",
       "        [442, 105, 484, 234],\n",
       "        [442, 105, 484, 234],\n",
       "        [442, 105, 484, 234]],\n",
       "\n",
       "       [[443, 104, 484, 233],\n",
       "        [443, 104, 484, 233],\n",
       "        [443, 104, 484, 233],\n",
       "        ...,\n",
       "        [442, 105, 484, 234],\n",
       "        [442, 105, 484, 234],\n",
       "        [442, 105, 484, 234]],\n",
       "\n",
       "       [[443, 104, 484, 233],\n",
       "        [443, 104, 484, 233],\n",
       "        [442, 104, 484, 233],\n",
       "        ...,\n",
       "        [442, 105, 484, 234],\n",
       "        [442, 105, 484, 234],\n",
       "        [442, 105, 484, 235]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[446, 102, 487, 235],\n",
       "        [443, 102, 484, 235],\n",
       "        [442, 101, 483, 234],\n",
       "        ...,\n",
       "        [440, 104, 477, 232],\n",
       "        [440, 104, 478, 232],\n",
       "        [441, 104, 478, 233]],\n",
       "\n",
       "       [[443, 102, 484, 235],\n",
       "        [442, 101, 483, 234],\n",
       "        [442, 101, 482, 234],\n",
       "        ...,\n",
       "        [440, 104, 478, 232],\n",
       "        [441, 104, 478, 233],\n",
       "        [441, 104, 478, 233]],\n",
       "\n",
       "       [[442, 101, 483, 234],\n",
       "        [442, 101, 482, 234],\n",
       "        [442, 101, 481, 234],\n",
       "        ...,\n",
       "        [441, 104, 478, 233],\n",
       "        [441, 104, 478, 233],\n",
       "        [440, 105, 477, 232]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindict['x_ppl_box'][range(7,90),:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21811962127685547\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "testdict = Boxes(loc_files_test[3:4], box_test_txt[3:4], 20, pad ='pre')\n",
    "end = time.time()\n",
    "print(end -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7638"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testdict['abnormal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_index = np.nonzero(testdict['abnormal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_index = np.where(testdict['abnormal'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normal_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ac5e925bfb98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'normal_index' is not defined"
     ]
    }
   ],
   "source": [
    "len(normal_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-154"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindict['x_ppl_box'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_test_x = testdict['x_ppl_box'][abnormal_index]\n",
    "abnormal_test_y = testdict['y_ppl_box'][abnormal_index]\n",
    "test_x = testdict['x_ppl_box'][normal_index]\n",
    "test_y = testdict['y_ppl_box'][normal_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = (traindict['x_ppl_box'] - traindict['x_ppl_box'].mean())/traindict['x_ppl_box'].std()\n",
    "# xx_test_abnorm = (abnormal_test_x - traindict['x_ppl_box'].mean())/traindict['x_ppl_box'].std()\n",
    "# xx_test = (test_x - traindict['x_ppl_box'].mean())/traindict['x_ppl_box'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yy = (traindict['y_ppl_box'] - traindict['x_ppl_box'].mean())/traindict['x_ppl_box'].std()\n",
    "# yy_test_abnorm = (abnormal_test_y - traindict['x_ppl_box'].mean())/traindict['x_ppl_box'].std()\n",
    "# yy_test = (test_y - traindict['x_ppl_box'].mean())/traindict['x_ppl_box'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max1 = traindict['x_ppl_box'].max()\n",
    "min1 = traindict['x_ppl_box'].min()\n",
    "xx = (traindict['x_ppl_box'] - min1)/(max1 - min1)\n",
    "xx_test_abnorm = (abnormal_test_x - min1)/(max1-min1)\n",
    "xx_test = (test_x - min1)/(max1-min1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = (traindict['y_ppl_box'] - min1)/(max1-min1)\n",
    "yy_test_abnorm = (abnormal_test_y - min1)/(max1-min1)\n",
    "yy_test = (test_y - min1)/(max1-min1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.097856477166822\n",
      "-0.2022367194780988\n",
      "0.7670083876980429\n",
      "0.10065237651444547\n"
     ]
    }
   ],
   "source": [
    "print(yy_test.max())\n",
    "print(yy_test.min())\n",
    "print(yy_test_abnorm.max())\n",
    "print(yy_test_abnorm.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "xx_train, xx_val,yy_train,yy_val = train_test_split(xx,yy, test_size = 0.1)\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((xx_train,yy_train))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((xx_val,yy_val))\n",
    "val_univariate = val_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files Loaded with 5 Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_files_train, loc_files_test, box_train_txt, box_test_txt = Files_Load()\n",
    "# start = time.time()\n",
    "# x_ppl_box_5, y_ppl_box_5, frame_ppl_id_5, video_file_5 = Boxes(loc_files_train, box_train_txt, 5, pad ='pre')\n",
    "# end = time.time()\n",
    "# print(end -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# x_ppl_box_test_5, y_ppl_box_test_5, frame_ppl_id_test_5, video_file_test_5 = Boxes(loc_files_test, box_test_txt, 5, pad ='pre')\n",
    "# end = time.time()\n",
    "# print(end -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx_5 = (x_ppl_box_5 - x_ppl_box_5.mean())/x_ppl_box_5.std()\n",
    "# xx_test_5 = (x_ppl_box_test_5 - x_ppl_box_test_5.mean())/x_ppl_box_test_5.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yy_5 = (y_ppl_box_5 - x_ppl_box_5.mean())/x_ppl_box_5.std()\n",
    "# yy_test_5 = (y_ppl_box_test_5 - x_ppl_box_5.mean())/x_ppl_box_5.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 20 Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_20 = keras.Sequential()\n",
    "lstm_20.add(keras.layers.InputLayer(input_shape=xx.shape[-2:]))\n",
    "lstm_20.add(keras.layers.LSTM(1,return_sequences =True ))\n",
    "lstm_20.add(keras.layers.LSTM(1, return_sequences=True) )\n",
    "lstm_20.add(keras.layers.LSTM(4) )\n",
    "lstm_20.add(keras.layers.Dense(4) )\n",
    "lstm_20.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 20, 1)             24        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 1)             12        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4)                 96        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 152\n",
      "Trainable params: 152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_20.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 427/3902 [==>...........................] - ETA: 38s - loss: 0.1160"
     ]
    }
   ],
   "source": [
    "# Some\n",
    "lstm_20_history_1= lstm_20.fit(train_univariate,validation_data = val_univariate  , epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,101), lstm_20_history.history['loss'],'b+', label='loss')\n",
    "plt.plot(np.arange(1,101), lstm_20_history.history['val_loss'], 'r+', label ='val loss')\n",
    "plt.legend()\n",
    "plt.title('LSTM, 20 Sequence Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,51), lstm_20_history.history['loss'],'b+', label='loss')\n",
    "plt.plot(np.arange(1,51), lstm_20_history.history['val_loss'], 'r+', label ='val loss')\n",
    "plt.legend()\n",
    "plt.title('LSTM, 20 Sequence Length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 5 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.LSTM(4,input_shape= xx_5.shape[-2:]))\n",
    "# model.add(keras.layers.Dense(1))\n",
    "# model = keras.Sequential([\n",
    "\n",
    "#     keras.layers.Flatten(input_shape=(28, 28)),\n",
    "#     keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "#     keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "# ])\n",
    "model2.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(x=xx_5, y = yy_5,validation_split=.1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,101), history2.history['loss'],'b*', label='loss')\n",
    "plt.plot(np.arange(1,101),history2.history['val_loss'], 'ro', label ='val loss')\n",
    "plt.legend()\n",
    "plt.title('LSTM, 5 Sequence Length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iou metric. Should probably scale back to normal. Might not matter if not scaled back to normal\n",
    "class IouMetric(keras.metrics.Metric):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.iou_avg = self.add_weight(\"iou_avg\", initializer=\"zeros\")\n",
    "        self.bb_intersection_over_union = iou_function()\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true,y_pred):\n",
    "        print('Here1')\n",
    "        y_true = math_ops.cast(y_true, tf.float32)\n",
    "        y_pred = math_ops.cast(y_pred, tf.float32)\n",
    "        print('here')\n",
    "        iou_calc =  self.bb_intersection_over_union(y_true,y_pred)\n",
    "        self.iou_avg.assign_add(iou_calc)\n",
    "        self.count.assign_add(1)\n",
    "    def result(self):\n",
    "        return self.iou_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_actual,y_pred):\n",
    "#     print(y_actual)\n",
    "    kb.print_tensor(y_actual.shape)\n",
    "    kb.print_tensor(y_pred)\n",
    "    custom_loss=kb.square(y_actual-y_pred)\n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0000005\n",
    "dense_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=xx.shape[-2:]),\n",
    "    keras.layers.Dense(4)\n",
    "])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "dense_model.compile(loss=\"mse\", optimizer=opt, metrics=bb_intersection_over_union)\n",
    "# dense_model.compile(loss=custom_loss, optimizer=opt)\n",
    "dense_model.compile(loss=bb_intersection_over_union, optimizer=opt, metrics = bb_intersection_over_union)\n",
    "\n",
    "# history1 = dense_model.fit(xx, yy,validation_split=.1, \n",
    "#                     epochs=15)\n",
    "# dense_history = dense_model.fit(train_univariate, epochs=5, validation_data =val_univariate  )\n",
    "dense_history = dense_model.fit(xx,yy, epochs=6 )\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model.evaluate(xx_test,yy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model.evaluate(xx_test_abnorm,yy_test_abnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_ppl_box.shape)\n",
    "print(yy[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,51), dense_history.history['loss'],'b+', label='loss')\n",
    "plt.plot(np.arange(1,51),dense_history.history['val_loss'], 'r+', label ='val loss')\n",
    "plt.legend()\n",
    "plt.title('Baseline Dense Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,7), dense_history.history['loss'],'b+', label='loss')\n",
    "plt.plot(np.arange(1,7), dense_history.history['val_loss'], 'r+', label ='val loss')\n",
    "plt.legend()\n",
    "plt.title('Baseline Dense Network Data Shuffled, lr = {}'.format(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dense_history.history['loss'][30:51])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i =2805\n",
    "j=2806"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(x=xx_test, y=yy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_test = (x_ppl_box_test - x_ppl_box_test.mean())/x_ppl_box_test.std()\n",
    "y_pred = model1.predict(xx_test[i:j])*x_ppl_box_test.std() + x_ppl_box_test.mean()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ppl_box_test[i:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ppl_id_test[i:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_test[i:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPlot(y_ppl_box_test[i:j],y_pred, frame_ppl_id_test[i:j], video_file_test[i:j], vid_type ='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
