{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "# %matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "# from tensorflow.python.ops import math_ops\n",
    "import tensorflow.keras.backend as kb\n",
    "# from kerastuner.tuners import RandomSearch\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.utils import normalize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Files_Load():\n",
    "    train_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Train_Box/\"\n",
    "    test_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Test_Box/\"\n",
    "    box_train_txt = os.listdir(train_file)\n",
    "    box_train_txt.sort()\n",
    "    box_test_txt = os.listdir(test_file)\n",
    "    box_test_txt.sort()\n",
    "    \n",
    "    loc_files_train, loc_files_test = [], []\n",
    "    \n",
    "    for txt in box_train_txt:\n",
    "        loc_files_train.append(train_file + txt)\n",
    "    for txt in box_test_txt:\n",
    "        loc_files_test.append(test_file + txt)\n",
    "    \n",
    "    return loc_files_train, loc_files_test, box_train_txt, box_test_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Boxes(loc_files, txt_names, time_steps, pad ='pre'):\n",
    "    \"\"\"\n",
    "    loc_files: List that contains that has text files save\n",
    "    txt_names: Txt file names. For visualization process\n",
    "    time_step: Sequence length input\n",
    "    pad: inputs 'pre' or 'post'\n",
    "    \n",
    "    x_person_box: Has bounding box locations\n",
    "    y_person_box: Label for bounding box locations\n",
    "    frame_person_id: Contains frame Number and person_Id of entire sequence, \n",
    "                     Last element is prediction frame. For visulization process\n",
    "    video_file: Points to video file used. For visulization process\n",
    "    \"\"\"\n",
    "    \n",
    "    x_ppl_box, y_ppl_box, frame_ppl_id, video_file, abnormal = [], [], [], [],[]  #Has bounding box locations inside\n",
    "    \n",
    "    #For splitting process\n",
    "    split_train_test = 0\n",
    "    split = 0\n",
    "    find_split = 0\n",
    "    \n",
    "    # Tells me how many in sequence was short.\n",
    "    # Do I want to go back and count for train and test seperatly \n",
    "    short_len = 0\n",
    "    \n",
    "#     datadict = OrderedDict()\n",
    "    datadict = {}\n",
    "    \n",
    "    for loc, txt_name in zip(loc_files, txt_names):\n",
    "        data = pd.read_csv(loc, ' ' )\n",
    "        # Note that person_box is 1 behind ID\n",
    "        max_person = data['Person_ID'].max()\n",
    "        for num in range(1,max_person+1):\n",
    "            temp_box = data[data['Person_ID'] == num ]['BB_tl_0\tBB_tl_1\tBB_br_0\tBB_br_1'.split()].values\n",
    "            person_seq_len = len(temp_box)\n",
    "            temp_frame_id = data[data['Person_ID'] == num ]['Frame_Number Person_ID'.split()].values\n",
    "            abnormal_frame_ped = data[data['Person_ID'] == num]['anomaly'].values\n",
    "            if person_seq_len > time_steps:\n",
    "                for i in range(0, person_seq_len - time_steps):\n",
    "                    temp_person_box = temp_box[i:(i+time_steps)]\n",
    "                    temp_fr_person_id = temp_frame_id[i:(i+time_steps+1)]\n",
    "\n",
    "                    x_ppl_box.append(temp_person_box)\n",
    "                    y_ppl_box.append(temp_box[i+time_steps])\n",
    "                    \n",
    "                    assert temp_person_box.shape == (time_steps,4)\n",
    "                    assert temp_fr_person_id.shape  == (time_steps+1,2), print(temp_fr_person_id.shape)\n",
    "                    \n",
    "                    frame_ppl_id.append(temp_fr_person_id)\n",
    "                    \n",
    "                    video_file.append(txt_name)      \n",
    "                    abnormal.append(abnormal_frame_ped[i+time_steps]) #Finds if predicted frame is abnormal\n",
    "                    \n",
    "            elif person_seq_len == 1:\n",
    "                # want it to skip loop\n",
    "                continue\n",
    "            elif person_seq_len <= time_steps:\n",
    "                temp_person_box_unpad = temp_box\n",
    "                temp_fr_person_id_unpad = temp_frame_id\n",
    "                temp_person_box = pad_sequences(temp_person_box_unpad.T, maxlen = time_steps+1, padding = pad).T\n",
    "                temp_fr_person_id = pad_sequences(temp_fr_person_id_unpad.T,  maxlen = time_steps+1, padding = pad).T\n",
    "                \n",
    "                assert temp_person_box.shape == (time_steps+1,4)\n",
    "                assert temp_fr_person_id.shape  == (time_steps+1,2)\n",
    "                \n",
    "                x_ppl_box.append(temp_person_box[0:time_steps,:])\n",
    "                y_ppl_box.append(temp_person_box[time_steps,:])\n",
    "                \n",
    "                frame_ppl_id.append(temp_fr_person_id[0:time_steps+1,:])\n",
    "                \n",
    "                video_file.append(txt_name)\n",
    "                abnormal.append(abnormal_frame_ped[-1]) #Finds if predicted frame is abnormal\n",
    "\n",
    "            else:\n",
    "                print('error')\n",
    "\n",
    "    \n",
    "    datadict['x_ppl_box'] = np.array(x_ppl_box)\n",
    "    datadict['y_ppl_box'] = np.array(y_ppl_box)\n",
    "    datadict['frame_ppl_id'] = np.array(frame_ppl_id)\n",
    "    datadict['video_file'] = np.array(video_file)\n",
    "    datadict['abnormal'] = np.array(abnormal)\n",
    "\n",
    "        \n",
    "    return  datadict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoPlot(y_ppl_box,y_pred, frame_ppl_id, video_file, vid_type ='test'):\n",
    "    \"\"\"\n",
    "    Function is plotting the ground truth and predicted \n",
    "    \n",
    "    y_ppl_box: Label for bounding box locations\n",
    "    y_ppl_box_pred: Prediction from Model\n",
    "    frame_person_id: Contains frame Number and person_Id of entire sequence, \n",
    "                     Last element is prediction frame. For visulization process\n",
    "    video_file: Points to video file used. For visulization process\n",
    "    \"\"\"\n",
    "    # Need a way to save images\n",
    "    file = {}\n",
    "    file['train'] = '/home/akanu/Dataset/Anomaly/Avenue_Dataset/training_videos/'\n",
    "    file['test'] = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/testing_videos/\"\n",
    "    loc_videos = file[vid_type] + video_file[0][:2] + '.' + video_file[0][2:5]\n",
    "    ###\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(loc_videos)\n",
    "    \n",
    "    for i in range(-1, frame_ppl_id[0,-1,0] + 1):\n",
    "        #Assune sequences are connected and don't skip cuz of occlusions \n",
    "        print(i)\n",
    "        ret, frame = video_capture.read()\n",
    "        if i == frame_ppl_id[0,-1,0]:\n",
    "            pred_frame = frame.copy()\n",
    "            cv2.rectangle(frame, (int(y_ppl_box[0,0]), int(y_ppl_box[0,1])), (int(y_ppl_box[0,2]), int(y_ppl_box[0,3])),(255,255,255), 2)\n",
    "            cv2.putText(frame, str(frame_ppl_id[0,-1,1]),(int(y_ppl_box[0,0]), int(y_ppl_box[0,1])),0, 5e-3 * 200, (0,255,0),2)\n",
    "\n",
    "            cv2.rectangle(pred_frame, (int(y_pred[0,0]), int(y_pred[0,1])), (int(y_pred[0,2]), int(y_pred[0,3])),(255,255,0), 2)\n",
    "            cv2.putText(pred_frame, str(frame_ppl_id[0,-1,1]),(int(y_pred[0,0]), int(y_pred[0,1])),0, 5e-3 * 200, (0,255,0),2)\n",
    "            \n",
    "            cv2.imwrite('/home/akanu/git/deep_sort_yolov3/Images_saved/pred.jpg', pred_frame)\n",
    "            cv2.imwrite(\"/home/akanu/git/deep_sort_yolov3/Images_saved/gt.jpg\", frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOU Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(y, x):\n",
    "    xA = kb.max((x[:,0:1],y[:,0:1]), axis=0,keepdims=True)\n",
    "    yA = kb.max((x[:,1:2],y[:,1:2]), axis=0,keepdims=True)\n",
    "    xB = kb.min((x[:,2:3],y[:,2:3]), axis=0,keepdims=True)\n",
    "    yB = kb.min((x[:,3:4],y[:,3:4]), axis=0,keepdims=True)\n",
    "\n",
    "    interArea1 = kb.max((kb.zeros_like(xB), (xB-xA +1) ), axis=0, keepdims=True)\n",
    "    interArea2 = kb.max((kb.zeros_like(xB), (yB-yA +1) ), axis=0, keepdims=True)\n",
    "    interArea = interArea1*interArea2\n",
    "    boxAArea = (x[:,2:3] - x[:,0:1] + 1) * (x[:,3:4] - x[:,1:2] + 1)\n",
    "    boxBArea = (y[:,2:3] - y[:,0:1] + 1) * (y[:,3:4] - y[:,1:2] + 1)\n",
    "\n",
    "    iou = interArea / (boxAArea + boxBArea - interArea)\n",
    "    iou_mean = kb.mean(iou)\n",
    "    return iou_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_frames = 20 #controls the amount of frames used to predict next bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.717153549194336\n"
     ]
    }
   ],
   "source": [
    "loc_files_train, loc_files_test, box_train_txt, box_test_txt = Files_Load()\n",
    "start = time.time()\n",
    "traindict = Boxes(loc_files_train, box_train_txt, len_frames, pad ='pre')\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize(traindict['x_ppl_box'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2163527011871338\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "testdict = Boxes(loc_files_test[3:4], box_test_txt[3:4], len_frames, pad ='pre')\n",
    "end = time.time()\n",
    "print(end -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_index = np.nonzero(testdict['abnormal'])\n",
    "normal_index = np.where(testdict['abnormal'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_test_x = testdict['x_ppl_box'][abnormal_index]\n",
    "abnormal_test_y = testdict['y_ppl_box'][abnormal_index]\n",
    "test_x = testdict['x_ppl_box'][normal_index]\n",
    "test_y = testdict['y_ppl_box'][normal_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max1 = traindict['x_ppl_box'].max()\n",
    "min1 = traindict['x_ppl_box'].min()\n",
    "xx = (traindict['x_ppl_box'] - min1)/(max1 - min1)\n",
    "xx_test_abnorm = (abnormal_test_x - min1)/(max1-min1)\n",
    "xx_test = (test_x - min1)/(max1-min1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = (traindict['y_ppl_box'] - min1)/(max1-min1)\n",
    "yy_test_abnorm = (abnormal_test_y - min1)/(max1-min1)\n",
    "yy_test = (test_y - min1)/(max1-min1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "test_size = 0.1\n",
    "xx_train, xx_val,yy_train,yy_val = train_test_split(xx,yy, test_size = test_size)\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((xx_train,yy_train))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((xx_val,yy_val))\n",
    "val_univariate = val_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \n",
    "    def Files_Load():\n",
    "        train_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Train_Box/\"\n",
    "        test_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Test_Box/\"\n",
    "        box_train_txt = os.listdir(train_file)\n",
    "        box_train_txt.sort()\n",
    "        box_test_txt = os.listdir(test_file)\n",
    "        box_test_txt.sort()\n",
    "\n",
    "        loc_files_train, loc_files_test = [], []\n",
    "\n",
    "        for txt in box_train_txt:\n",
    "            loc_files_train.append(train_file + txt)\n",
    "        for txt in box_test_txt:\n",
    "            loc_files_test.append(test_file + txt)\n",
    "\n",
    "        return loc_files_train, loc_files_test, box_train_txt, box_test_txt\n",
    "\n",
    "    \n",
    "    def Boxes(loc_files, txt_names, time_steps, pad ='pre'):\n",
    "        \"\"\"\n",
    "        loc_files: List that contains that has text files save\n",
    "        txt_names: Txt file names. For visualization process\n",
    "        time_step: Sequence length input\n",
    "        pad: inputs 'pre' or 'post'\n",
    "\n",
    "        x_person_box: Has bounding box locations\n",
    "        y_person_box: Label for bounding box locations\n",
    "        frame_person_id: Contains frame Number and person_Id of entire sequence, \n",
    "                         Last element is prediction frame. For visulization process\n",
    "        video_file: Points to video file used. For visulization process\n",
    "        \"\"\"\n",
    "\n",
    "        x_ppl_box, y_ppl_box, frame_ppl_id, video_file, abnormal = [], [], [], [],[]  #Has bounding box locations inside\n",
    "\n",
    "        #For splitting process\n",
    "        split_train_test = 0\n",
    "        split = 0\n",
    "        find_split = 0\n",
    "\n",
    "        # Tells me how many in sequence was short.\n",
    "        # Do I want to go back and count for train and test seperatly \n",
    "        short_len = 0\n",
    "\n",
    "    #     datadict = OrderedDict()\n",
    "        datadict = {}\n",
    "\n",
    "        for loc, txt_name in zip(loc_files, txt_names):\n",
    "            data = pd.read_csv(loc, ' ' )\n",
    "            # Note that person_box is 1 behind ID\n",
    "            max_person = data['Person_ID'].max()\n",
    "            for num in range(1,max_person+1):\n",
    "                temp_box = data[data['Person_ID'] == num ]['BB_tl_0\tBB_tl_1\tBB_br_0\tBB_br_1'.split()].values\n",
    "                person_seq_len = len(temp_box)\n",
    "                temp_frame_id = data[data['Person_ID'] == num ]['Frame_Number Person_ID'.split()].values\n",
    "                abnormal_frame_ped = data[data['Person_ID'] == num]['anomaly'].values\n",
    "                if person_seq_len > time_steps:\n",
    "                    for i in range(0, person_seq_len - time_steps):\n",
    "                        temp_person_box = temp_box[i:(i+time_steps)]\n",
    "                        temp_fr_person_id = temp_frame_id[i:(i+time_steps+1)]\n",
    "\n",
    "                        x_ppl_box.append(temp_person_box)\n",
    "                        y_ppl_box.append(temp_box[i+time_steps])\n",
    "\n",
    "                        assert temp_person_box.shape == (time_steps,4)\n",
    "                        assert temp_fr_person_id.shape  == (time_steps+1,2), print(temp_fr_person_id.shape)\n",
    "\n",
    "                        frame_ppl_id.append(temp_fr_person_id)\n",
    "\n",
    "                        video_file.append(txt_name)      \n",
    "                        abnormal.append(abnormal_frame_ped[i+time_steps]) #Finds if predicted frame is abnormal\n",
    "\n",
    "                elif person_seq_len == 1:\n",
    "                    # want it to skip loop\n",
    "                    continue\n",
    "                elif person_seq_len <= time_steps:\n",
    "                    temp_person_box_unpad = temp_box\n",
    "                    temp_fr_person_id_unpad = temp_frame_id\n",
    "                    temp_person_box = pad_sequences(temp_person_box_unpad.T, maxlen = time_steps+1, padding = pad).T\n",
    "                    temp_fr_person_id = pad_sequences(temp_fr_person_id_unpad.T,  maxlen = time_steps+1, padding = pad).T\n",
    "\n",
    "                    assert temp_person_box.shape == (time_steps+1,4)\n",
    "                    assert temp_fr_person_id.shape  == (time_steps+1,2)\n",
    "\n",
    "                    x_ppl_box.append(temp_person_box[0:time_steps,:])\n",
    "                    y_ppl_box.append(temp_person_box[time_steps,:])\n",
    "\n",
    "                    frame_ppl_id.append(temp_fr_person_id[0:time_steps+1,:])\n",
    "\n",
    "                    video_file.append(txt_name)\n",
    "                    abnormal.append(abnormal_frame_ped[-1]) #Finds if predicted frame is abnormal\n",
    "\n",
    "                else:\n",
    "                    print('error')\n",
    "\n",
    "\n",
    "        datadict['x_ppl_box'] = np.array(x_ppl_box)\n",
    "        datadict['y_ppl_box'] = np.array(y_ppl_box)\n",
    "        datadict['frame_ppl_id'] = np.array(frame_ppl_id)\n",
    "        datadict['video_file'] = np.array(video_file)\n",
    "        datadict['abnormal'] = np.array(abnormal)\n",
    "\n",
    "\n",
    "        return  datadict    \n",
    "    \n",
    "    \n",
    "\n",
    "    len_frames =20\n",
    "    \n",
    "    loc_files_train, loc_files_test, box_train_txt, box_test_txt = Files_Load()\n",
    "    traindict = Boxes(loc_files_train, box_train_txt, len_frames, pad ='pre')\n",
    "    testdict = Boxes(loc_files_test[3:4], box_test_txt[3:4], len_frames, pad ='pre')\n",
    "    \n",
    "\n",
    "    max1 = traindict['x_ppl_box'].max()\n",
    "    min1 = traindict['x_ppl_box'].min()\n",
    "    x_train = (traindict['x_ppl_box'] - min1)/(max1 - min1)\n",
    "    y_train = (traindict['y_ppl_box'] - min1)/(max1-min1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_x = testdict['x_ppl_box']\n",
    "    test_y = testdict['y_ppl_box']    \n",
    "    x_test = (test_x - min1)/(max1-min1)\n",
    "    y_test = (test_y - min1)/(max1-min1)\n",
    "\n",
    "\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx,yy,xx_test,yy_test = data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_lstm_model(n_hidden =2, n_neurons=1, learning_rate =5e-3, input_shape = xx.shape[-2:]):\n",
    "#     lstm_model = keras.models.Sequential()\n",
    "# #     lstm_model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "#     lstm_model.add(keras.layers.LSTM(n_neurons,return_sequences =True, input_shape=input_shape))\n",
    "#     for layer in range(n_hidden):\n",
    "#         lstm_model.add(keras.layers.LSTM({{choice([1, 2,3,4,5,6,7,8,9,10])}},return_sequences =True))\n",
    "#     lstm_model.add(keras.layers.LSTM(4) )\n",
    "#     lstm_model.add(keras.layers.Dense(4) )\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate={{uniform(1e-, 1)}})\n",
    "#     lstm_model.compile(loss='mse', optimizer=opt, metrics = bb_intersection_over_union)\n",
    "#     return lstm_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train,y_train,x_test,y_test):\n",
    "    def bb_intersection_over_union(y, x):\n",
    "        xA = kb.max((x[:,0:1],y[:,0:1]), axis=0,keepdims=True)\n",
    "        yA = kb.max((x[:,1:2],y[:,1:2]), axis=0,keepdims=True)\n",
    "        xB = kb.min((x[:,2:3],y[:,2:3]), axis=0,keepdims=True)\n",
    "        yB = kb.min((x[:,3:4],y[:,3:4]), axis=0,keepdims=True)\n",
    "\n",
    "        interArea1 = kb.max((kb.zeros_like(xB), (xB-xA +1) ), axis=0, keepdims=True)\n",
    "        interArea2 = kb.max((kb.zeros_like(xB), (yB-yA +1) ), axis=0, keepdims=True)\n",
    "        interArea = interArea1*interArea2\n",
    "        boxAArea = (x[:,2:3] - x[:,0:1] + 1) * (x[:,3:4] - x[:,1:2] + 1)\n",
    "        boxBArea = (y[:,2:3] - y[:,0:1] + 1) * (y[:,3:4] - y[:,1:2] + 1)\n",
    "\n",
    "        iou = interArea / (boxAArea + boxBArea - interArea)\n",
    "        iou_mean = kb.mean(iou)\n",
    "        return iou_mean\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    lstm_model = keras.models.Sequential()\n",
    "#     lstm_model.add(keras.layers.InputLayer(input_shape=x_train.shape[-2:]))\n",
    "    lstm_model.add(keras.layers.LSTM({{choice([2,10])}},return_sequences =True, input_shape=x_train.shape[-2:]))\n",
    "    n_hidden =5\n",
    "    for layer in range(n_hidden):\n",
    "        lstm_model.add(keras.layers.LSTM({{choice([2,10])}},return_sequences =True))\n",
    "    lstm_model.add(keras.layers.LSTM(4) )\n",
    "    lstm_model.add(keras.layers.Dense(4) )\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate={{choice([1e-7, 1e-5])}})\n",
    "    lstm_model.compile(loss='mse', optimizer=opt, metrics = bb_intersection_over_union)\n",
    "    lstm_history = lstm_model.fit(x_train, y_train, \n",
    "                                  batch_size= 32, \n",
    "                                  epochs=2,\n",
    "                                  verbose=2,\n",
    "                                  validation_split =0.1)\n",
    "#     iou = np.amax(lstm_history.history['bb_intersection_over_union'])\n",
    "    iou = np.amax(lstm_history.history['loss'])\n",
    "\n",
    "    print('Best iou acc of epoch:', iou)\n",
    "    return {'loss': -iou, 'status': STATUS_OK, 'lstm_model': lstm_model }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.backend.clear_session()\n",
    "# keras_reg = keras.wrappers.scikit_learn.KerasRegressor(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_reg.fit(xx_train,yy_train,epochs=2, \n",
    "#               validation_data = (xx_val, yy_val),\n",
    "#              callbacks =[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import cv2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import MinMaxScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from collections import OrderedDict\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow.keras.backend as kb\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import normalize\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'LSTM': hp.choice('LSTM', [2,10]),\n",
      "        'LSTM_1': hp.choice('LSTM_1', [2,10]),\n",
      "        'learning_rate': hp.choice('learning_rate', [1e-7, 1e-5]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \n",
      "   3: def Files_Load():\n",
      "   4:     train_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Train_Box/\"\n",
      "   5:     test_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Test_Box/\"\n",
      "   6:     box_train_txt = os.listdir(train_file)\n",
      "   7:     box_train_txt.sort()\n",
      "   8:     box_test_txt = os.listdir(test_file)\n",
      "   9:     box_test_txt.sort()\n",
      "  10: \n",
      "  11:     loc_files_train, loc_files_test = [], []\n",
      "  12: \n",
      "  13:     for txt in box_train_txt:\n",
      "  14:         loc_files_train.append(train_file + txt)\n",
      "  15:     for txt in box_test_txt:\n",
      "  16:         loc_files_test.append(test_file + txt)\n",
      "  17: \n",
      "  18:     return loc_files_train, loc_files_test, box_train_txt, box_test_txt\n",
      "  19: \n",
      "  20: \n",
      "  21: def Boxes(loc_files, txt_names, time_steps, pad ='pre'):\n",
      "  22:     \"\"\"\n",
      "  23:     loc_files: List that contains that has text files save\n",
      "  24:     txt_names: Txt file names. For visualization process\n",
      "  25:     time_step: Sequence length input\n",
      "  26:     pad: inputs 'pre' or 'post'\n",
      "  27: \n",
      "  28:     x_person_box: Has bounding box locations\n",
      "  29:     y_person_box: Label for bounding box locations\n",
      "  30:     frame_person_id: Contains frame Number and person_Id of entire sequence, \n",
      "  31:                      Last element is prediction frame. For visulization process\n",
      "  32:     video_file: Points to video file used. For visulization process\n",
      "  33:     \"\"\"\n",
      "  34: \n",
      "  35:     x_ppl_box, y_ppl_box, frame_ppl_id, video_file, abnormal = [], [], [], [],[]  #Has bounding box locations inside\n",
      "  36: \n",
      "  37:     #For splitting process\n",
      "  38:     split_train_test = 0\n",
      "  39:     split = 0\n",
      "  40:     find_split = 0\n",
      "  41: \n",
      "  42:     # Tells me how many in sequence was short.\n",
      "  43:     # Do I want to go back and count for train and test seperatly \n",
      "  44:     short_len = 0\n",
      "  45: \n",
      "  46: #     datadict = OrderedDict()\n",
      "  47:     datadict = {}\n",
      "  48: \n",
      "  49:     for loc, txt_name in zip(loc_files, txt_names):\n",
      "  50:         data = pd.read_csv(loc, ' ' )\n",
      "  51:         # Note that person_box is 1 behind ID\n",
      "  52:         max_person = data['Person_ID'].max()\n",
      "  53:         for num in range(1,max_person+1):\n",
      "  54:             temp_box = data[data['Person_ID'] == num ]['BB_tl_0\tBB_tl_1\tBB_br_0\tBB_br_1'.split()].values\n",
      "  55:             person_seq_len = len(temp_box)\n",
      "  56:             temp_frame_id = data[data['Person_ID'] == num ]['Frame_Number Person_ID'.split()].values\n",
      "  57:             abnormal_frame_ped = data[data['Person_ID'] == num]['anomaly'].values\n",
      "  58:             if person_seq_len > time_steps:\n",
      "  59:                 for i in range(0, person_seq_len - time_steps):\n",
      "  60:                     temp_person_box = temp_box[i:(i+time_steps)]\n",
      "  61:                     temp_fr_person_id = temp_frame_id[i:(i+time_steps+1)]\n",
      "  62: \n",
      "  63:                     x_ppl_box.append(temp_person_box)\n",
      "  64:                     y_ppl_box.append(temp_box[i+time_steps])\n",
      "  65: \n",
      "  66:                     assert temp_person_box.shape == (time_steps,4)\n",
      "  67:                     assert temp_fr_person_id.shape  == (time_steps+1,2), print(temp_fr_person_id.shape)\n",
      "  68: \n",
      "  69:                     frame_ppl_id.append(temp_fr_person_id)\n",
      "  70: \n",
      "  71:                     video_file.append(txt_name)      \n",
      "  72:                     abnormal.append(abnormal_frame_ped[i+time_steps]) #Finds if predicted frame is abnormal\n",
      "  73: \n",
      "  74:             elif person_seq_len == 1:\n",
      "  75:                 # want it to skip loop\n",
      "  76:                 continue\n",
      "  77:             elif person_seq_len <= time_steps:\n",
      "  78:                 temp_person_box_unpad = temp_box\n",
      "  79:                 temp_fr_person_id_unpad = temp_frame_id\n",
      "  80:                 temp_person_box = pad_sequences(temp_person_box_unpad.T, maxlen = time_steps+1, padding = pad).T\n",
      "  81:                 temp_fr_person_id = pad_sequences(temp_fr_person_id_unpad.T,  maxlen = time_steps+1, padding = pad).T\n",
      "  82: \n",
      "  83:                 assert temp_person_box.shape == (time_steps+1,4)\n",
      "  84:                 assert temp_fr_person_id.shape  == (time_steps+1,2)\n",
      "  85: \n",
      "  86:                 x_ppl_box.append(temp_person_box[0:time_steps,:])\n",
      "  87:                 y_ppl_box.append(temp_person_box[time_steps,:])\n",
      "  88: \n",
      "  89:                 frame_ppl_id.append(temp_fr_person_id[0:time_steps+1,:])\n",
      "  90: \n",
      "  91:                 video_file.append(txt_name)\n",
      "  92:                 abnormal.append(abnormal_frame_ped[-1]) #Finds if predicted frame is abnormal\n",
      "  93: \n",
      "  94:             else:\n",
      "  95:                 print('error')\n",
      "  96: \n",
      "  97: \n",
      "  98:     datadict['x_ppl_box'] = np.array(x_ppl_box)\n",
      "  99:     datadict['y_ppl_box'] = np.array(y_ppl_box)\n",
      " 100:     datadict['frame_ppl_id'] = np.array(frame_ppl_id)\n",
      " 101:     datadict['video_file'] = np.array(video_file)\n",
      " 102:     datadict['abnormal'] = np.array(abnormal)\n",
      " 103: \n",
      " 104: \n",
      " 105:     return  datadict    \n",
      " 106: \n",
      " 107: \n",
      " 108: \n",
      " 109: len_frames =20\n",
      " 110: \n",
      " 111: loc_files_train, loc_files_test, box_train_txt, box_test_txt = Files_Load()\n",
      " 112: traindict = Boxes(loc_files_train, box_train_txt, len_frames, pad ='pre')\n",
      " 113: testdict = Boxes(loc_files_test[3:4], box_test_txt[3:4], len_frames, pad ='pre')\n",
      " 114: \n",
      " 115: \n",
      " 116: max1 = traindict['x_ppl_box'].max()\n",
      " 117: min1 = traindict['x_ppl_box'].min()\n",
      " 118: x_train = (traindict['x_ppl_box'] - min1)/(max1 - min1)\n",
      " 119: y_train = (traindict['y_ppl_box'] - min1)/(max1-min1)\n",
      " 120: \n",
      " 121: \n",
      " 122: \n",
      " 123: test_x = testdict['x_ppl_box']\n",
      " 124: test_y = testdict['y_ppl_box']    \n",
      " 125: x_test = (test_x - min1)/(max1-min1)\n",
      " 126: y_test = (test_y - min1)/(max1-min1)\n",
      " 127: \n",
      " 128: \n",
      " 129: \n",
      " 130: \n",
      " 131: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     def bb_intersection_over_union(y, x):\n",
      "   4:         xA = kb.max((x[:,0:1],y[:,0:1]), axis=0,keepdims=True)\n",
      "   5:         yA = kb.max((x[:,1:2],y[:,1:2]), axis=0,keepdims=True)\n",
      "   6:         xB = kb.min((x[:,2:3],y[:,2:3]), axis=0,keepdims=True)\n",
      "   7:         yB = kb.min((x[:,3:4],y[:,3:4]), axis=0,keepdims=True)\n",
      "   8: \n",
      "   9:         interArea1 = kb.max((kb.zeros_like(xB), (xB-xA +1) ), axis=0, keepdims=True)\n",
      "  10:         interArea2 = kb.max((kb.zeros_like(xB), (yB-yA +1) ), axis=0, keepdims=True)\n",
      "  11:         interArea = interArea1*interArea2\n",
      "  12:         boxAArea = (x[:,2:3] - x[:,0:1] + 1) * (x[:,3:4] - x[:,1:2] + 1)\n",
      "  13:         boxBArea = (y[:,2:3] - y[:,0:1] + 1) * (y[:,3:4] - y[:,1:2] + 1)\n",
      "  14: \n",
      "  15:         iou = interArea / (boxAArea + boxBArea - interArea)\n",
      "  16:         iou_mean = kb.mean(iou)\n",
      "  17:         return iou_mean\n",
      "  18:     \n",
      "  19:     \n",
      "  20:     \n",
      "  21:     \n",
      "  22:     lstm_model = keras.models.Sequential()\n",
      "  23: #     lstm_model.add(keras.layers.InputLayer(input_shape=x_train.shape[-2:]))\n",
      "  24:     lstm_model.add(keras.layers.LSTM(space['LSTM'],return_sequences =True, input_shape=x_train.shape[-2:]))\n",
      "  25:     n_hidden =5\n",
      "  26:     for layer in range(n_hidden):\n",
      "  27:         lstm_model.add(keras.layers.LSTM(space['LSTM_1'],return_sequences =True))\n",
      "  28:     lstm_model.add(keras.layers.LSTM(4) )\n",
      "  29:     lstm_model.add(keras.layers.Dense(4) )\n",
      "  30:     opt = tf.keras.optimizers.Adam(learning_rate=space['learning_rate'])\n",
      "  31:     lstm_model.compile(loss='mse', optimizer=opt, metrics = bb_intersection_over_union)\n",
      "  32:     lstm_history = lstm_model.fit(x_train, y_train, \n",
      "  33:                                   batch_size= 32, \n",
      "  34:                                   epochs=2,\n",
      "  35:                                   verbose=2,\n",
      "  36:                                   validation_split =0.1)\n",
      "  37: #     iou = np.amax(lstm_history.history['bb_intersection_over_union'])\n",
      "  38:     iou = np.amax(lstm_history.history['loss'])\n",
      "  39: \n",
      "  40:     print('Best iou acc of epoch:', iou)\n",
      "  41:     return {'loss': -iou, 'status': STATUS_OK, 'lstm_model': lstm_model }\n",
      "  42: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:11<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a68067b54544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                           \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                           notebook_name='Fine_Tuning_Neural_Network_Hyperparameters')\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    137\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m              return_argmin=True),\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         )\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         )\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/anomalous_pred/temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    795\u001b[0m           data_adapter.train_validation_split((x, y, sample_weight),\n\u001b[1;32m    796\u001b[0m                                               \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                                               shuffle=False))\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   train_arrays = nest.map_structure(\n\u001b[0;32m-> 1338\u001b[0;31m       functools.partial(_split, indices=train_indices), arrays)\n\u001b[0m\u001b[1;32m   1339\u001b[0m   val_arrays = nest.map_structure(\n\u001b[1;32m   1340\u001b[0m       functools.partial(_split, indices=val_indices), arrays)\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_split\u001b[0;34m(t, indices)\u001b[0m\n\u001b[1;32m   1333\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   train_arrays = nest.map_structure(\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   4539\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4540\u001b[0m       \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4541\u001b[0;31m       batch_dims=batch_dims)\n\u001b[0m\u001b[1;32m   4542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4522\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4524\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   3753\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3754\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3755\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3756\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3757\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbatch_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]"
     ]
    }
   ],
   "source": [
    "# x_train, y_train, x_test, y_test = data()\n",
    "\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=1,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='Fine_Tuning_Neural_Network_Hyperparameters')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(x_train,y_train, x_test,y_test):\n",
    "    model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=x_train.shape[-2:]),\n",
    "    keras.layers.Dense({{choice([20,50,100])}})\n",
    "    ])\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    model.compile(loss=\"mse\", optimizer=opt, metrics=['accuracy'])\n",
    "    result = model.fit(x_train,y_train,\n",
    "                      batch_size= 32,\n",
    "                      epochs =2,\n",
    "                      validation_split=0.1)\n",
    "    validation_acc = np.amax(result.history['val_acc'])\n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
