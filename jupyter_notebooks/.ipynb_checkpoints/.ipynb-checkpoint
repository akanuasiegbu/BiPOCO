{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os, sys, time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from collections import OrderedDict\n",
    "# from tensorflow.python.ops import math_ops\n",
    "# import tensorflow.keras.backend as kb\n",
    "from tensorflow_addons.utils.ensure_tf_install import _check_tf_version\n",
    "_check_tf_version()\n",
    "from tensorflow_addons import losses\n",
    "\n",
    "# Custom Functions\n",
    "# Added to path by using sys.path.append('')\n",
    "sys.path.append('/home/akanu/git/anomalous_pred/custom_functions')\n",
    "from load_data import Files_Load, Boxes, test_split_norm_abnorm, norm_train_max_min\n",
    "from custom_metrics import bb_intersection_over_union, bb_intersection_over_union_np\n",
    "from coordinate_change import xywh_tlbr, tlbr_xywh\n",
    "from pedsort import pedsort\n",
    "from visualize import visual_ouput\n",
    "\n",
    "# Kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Need to figue out why this is not working\n",
    "# from persistence1d import RunPersistence\n",
    "\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "#plt.style.use(['dark_background'])\n",
    "# plt.style.use('ggplot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "#   try:\n",
    "#     tf.config.experimental.set_virtual_device_configuration(\n",
    "#         gpus[1],\n",
    "#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5500)])\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Virtual devices must be set before GPUs have been initialized\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = 20\n",
    "startvid=0\n",
    "endvid=1\n",
    "\n",
    "train_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Train_Box/\"\n",
    "test_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Test_Box/\"\n",
    "\n",
    "loc_files_train, loc_files_test, box_train_txt, box_test_txt = Files_Load(train_file, test_file)\n",
    "\n",
    "# Don't forget to change to xywh \n",
    "traindict = Boxes(loc_files_train, box_train_txt, frames, pad ='pre', to_xywh=True)\n",
    "testdict = Boxes(loc_files_test[startvid:endvid], box_test_txt[startvid:endvid], frames, pad ='pre', to_xywh = True)\n",
    "abnormal_dict, normal_dict = test_split_norm_abnorm(testdict) #splits by which predicted frames are normal or not\n",
    "\n",
    "# Normilize data\n",
    "max1 = traindict['x_ppl_box'].max()\n",
    "min1 = traindict['x_ppl_box'].min()\n",
    "xx,yy = norm_train_max_min(data_dict = traindict, max1=max1,min1=min1)\n",
    "xx_norm,yy_norm = norm_train_max_min(data_dict = normal_dict, max1=max1,min1=min1)\n",
    "xx_abnorm,yy_abnorm = norm_train_max_min(data_dict = abnormal_dict, max1=max1,min1=min1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "xx_train, xx_val,yy_train,yy_val = train_test_split(xx,yy, test_size = 0.1)\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((xx_train,yy_train))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((xx_val,yy_val))\n",
    "val_univariate = val_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    lstm_20 = keras.Sequential()\n",
    "    lstm_20.add(keras.layers.InputLayer(input_shape=xx.shape[-2:]))\n",
    "    lstm_20.add(keras.layers.LSTM(4,return_sequences =True ))\n",
    "    lstm_20.add(keras.layers.LSTM(3,return_sequences =True ))\n",
    "    lstm_20.add(keras.layers.LSTM(6,return_sequences =True ))\n",
    "    lstm_20.add(keras.layers.LSTM(4,return_sequences =True ))\n",
    "    lstm_20.add(keras.layers.LSTM(4,return_sequences =True ))\n",
    "    lstm_20.add(keras.layers.LSTM(4) )\n",
    "    lstm_20.add(keras.layers.Dense(4) )\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=8.726e-06)\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\"lstm_5_xywh.h5\", \n",
    "                                                       save_best_only = True)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00005, patience=5)\n",
    "#     lstm_20.compile(optimizer=opt, loss=losses.GIoULoss(), metrics=bb_intersection_over_union)\n",
    "    # if use iou metric need to conver to tlbr \n",
    "    lstm_20.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "    lstm_20_history_1= lstm_20.fit(train_univariate,\n",
    "                               validation_data = val_univariate,\n",
    "                               epochs=100,\n",
    "                               callbacks = [early_stopping, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model (If model is trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('lstm_5_xywh.h5', custom_objects = {'loss':'mse'} , compile=True)\n",
    "lstm_20 = model\n",
    "# Appears because of my custom loss fuction I could use evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lstm_20.evaluate(xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check IOU for video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should make a for loop to evalute the table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_loc = \"/home/akanu/git/anomalous_pred/resulting_images/xywh/test_abnorm/{}_{}_{}\"\n",
    "visual_ouput(model=lstm_20,max1 = max1, min1=min1, vid=1,pic_loc =pic_loc, output_dict=abnormal_dict,xywh = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_loc = \"/home/akanu/git/anomalous_pred/resulting_images/xywh/test_norm/{}_{}_{}\"\n",
    "visual_ouput(model=lstm_20,max1 = max1, min1=min1, vid=1,pic_loc =pic_loc, output_dict=normal_dict,xywh = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "j ='1'\n",
    "ped = pedsort(testdict)\n",
    "x,y = norm_train_max_min(data_dict=ped[j], max1 = max1, min1=min1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_20' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-85c41338b090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbb_intersection_over_union_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxywh_tlbr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxywh_tlbr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame_ppl_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm_20' is not defined"
     ]
    }
   ],
   "source": [
    "out1 = lstm_20.predict(x)\n",
    "\n",
    "\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1), xywh_tlbr(y))\n",
    "plt.plot(ped[j]['frame_ppl_id'][:,-1,0], np.squeeze(out),'-')\n",
    "plt.plot(ped[j]['frame_ppl_id'][:,-1,0], ped[j]['abnormal'])\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('IOU')\n",
    "print(np.mean(out))\n",
    "# sns.distplot(np.squeeze(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using averaged iou to find Abnormal vs normal\n",
    "## Doesn't work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tries to find normal/abnormal pedestrian from averaged iou\n",
    "iou_sum = []\n",
    "for j in ped.keys():\n",
    "#     print(j)\n",
    "    x,y = norm_train_max_min(data_dict=ped[j], max1 = max1, min1=min1 )\n",
    "    out1 = lstm_20.predict(x)\n",
    "    out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "#     print(out.shape)\n",
    "    iou_sum.append([np.mean(out), np.any(ped[j]['abnormal']) ])\n",
    "iou_sum = np.array(iou_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_ped = iou_sum[np.nonzero(iou_sum[:,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ped = iou_sum[np.where(iou_sum[:,1]==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(norm_ped[:,0],'-*', label='normal')\n",
    "plt.plot(abnormal_ped[:,0], '-+', label='abnormal')\n",
    "plt.xlabel('pedestrian')\n",
    "plt.ylabel('Averaged IOU over time')\n",
    "plt.legend()\n",
    "# sns.distplot(norm_ped[:,0])\n",
    "# sns.distplot(abnormal_ped[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding distribution of normal vs abnormal pedestrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_abnorm = []\n",
    "iou_norm = []\n",
    "for j in ped.keys():\n",
    "#     print(j)\n",
    "    x,y = norm_train_max_min(data_dict=ped[j], max1 = max1, min1=min1 )\n",
    "    out1 = lstm_20.predict(x)\n",
    "    out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "#     print(out.shape)\n",
    "    if np.any(ped[j]['abnormal']):\n",
    "#         iou_abnorm.append(np.squeeze(out).tolist()[:])\n",
    "        iou_abnorm += np.squeeze(out).tolist()\n",
    "    else:\n",
    "#         iou_norm.append(np.squeeze(out).tolist())\n",
    "        iou_norm += np.squeeze(out).tolist()\n",
    "\n",
    "# iou_abnorm = np.array(iou_abnorm)\n",
    "# iou_norm = np.array(iou_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(iou_abnorm, label='abnormal pedestrain')\n",
    "sns.distplot(iou_norm, label='normal pedestrain')\n",
    "plt.title('Test Video 1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testdict['abnormal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding distrubtions of Normal vs abnormal frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startvid =0\n",
    "endvid=21\n",
    "\n",
    "testdict = Boxes(loc_files_test[startvid:endvid], box_test_txt[startvid:endvid], frames, pad ='pre', to_xywh = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out1 = lstm_20.predict(x)\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "out = np.squeeze(out)\n",
    "#     print(out.shape)\n",
    "norm_index = np.where(testdict['abnormal']== 0)\n",
    "abnorm_index = np.where(testdict['abnormal']== 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abnorm_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(out[abnorm_index], label='abnormal frames')\n",
    "sns.distplot(out[norm_index], label='normal frames')\n",
    "plt.title('All Test Video ')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification on frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out1 = lstm_20.predict(x)\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "out = np.squeeze(out)\n",
    "np.random.seed(49)\n",
    "rand = np.random.permutation(len(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # create model\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(1, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, out[rand], testdict['abnormal'][rand], cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline: %.2f%% (%.8f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifcation Even Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startvid =0\n",
    "endvid=21\n",
    "\n",
    "testdict = Boxes(loc_files_test[startvid:endvid], box_test_txt[startvid:endvid], frames, pad ='pre', to_xywh = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out1 = lstm_20.predict(x)\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "out = np.squeeze(out)\n",
    "np.random.seed(49)\n",
    "rand = np.random.permutation(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnorm_index = np.where(testdict['abnormal']==1)\n",
    "norm_index = np.where(testdict['abnormal'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abnormal: {}, normal: {}'.format(len(abnorm_index[0]), len(norm_index[0])))\n",
    "# Want to create a function that allows me to vary the amount of abnormalites in the trainingg loader\n",
    "# Want to also be able to vary amount of norms in training loader\n",
    "# Need to keep track of indexes so that I can assign correct ones to abnormal truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vary_abnormal_normal_frames():\n",
    "    abnorm_index = np.where(testdict['abnormal']==1)\n",
    "    norm_index = np.where(testdict['abnormal'] == 0)\n",
    "    abnormal_split = 0.5\n",
    "    val_split = 0.3\n",
    "    \n",
    "    ratio = int(len(abnorm_index[0])*abnormal_split)\n",
    "    a = abnorm_index[0][:ratio]\n",
    "    b = norm_index[0][:ratio*weight_scale]\n",
    "    \n",
    "    val_ratio = int(val_split*len(a) )\n",
    "    \n",
    "    val_a = a[:val_ratio]\n",
    "    val_b = b[:val_ratio*weight_scale]\n",
    "    \n",
    "    test_a = a[val_ratio:]\n",
    "    test_b = b[val_ratio*weight_scale:]\n",
    "    \n",
    "    c = abnorm_index[0][ratio:]\n",
    "    d = norm_index[0][ratio*weight_scale:]\n",
    "    \n",
    "    index = [a,b,c,d]\n",
    "#     train  = {}\n",
    "#     test = {}\n",
    "#     for i in index[:2]:\n",
    "#         x = out[i]\n",
    "#         y = testdict['abnormal'][ i]\n",
    "#         data.append\n",
    "        \n",
    "#     for i in index[2:]:\n",
    "#         x = out[i]\n",
    "#         y = testdict['abnormal'][ i]\n",
    "#         data.append\n",
    "        \n",
    "    \n",
    "    \n",
    "    train_x_an = out[ test_a]\n",
    "    train_y_an= testdict['abnormal'][ test_a ]\n",
    "    \n",
    "    train_x_n = out[ test_b]\n",
    "    train_y_n = testdict['abnormal'][ test_b ]\n",
    "    \n",
    "    train_x = np.append(train_x_an,train_x_n,axis =0)\n",
    "    train_y = np.append(train_y_an,train_y_n, axis = 0 )\n",
    "    \n",
    "    \n",
    "    val_x_an = out[ val_a]\n",
    "    val_y_an= testdict['abnormal'][ val_a ]\n",
    "    \n",
    "    val_x_n = out[ val_b]\n",
    "    val_y_n = testdict['abnormal'][ val_b ]\n",
    "    \n",
    "    val_x = np.append(val_x_an,val_x_n,axis =0)\n",
    "    val_y = np.append(val_y_an,val_y_n, axis = 0 )\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_x_an = out[ c ]\n",
    "    test_y_an= testdict['abnormal'][ c]\n",
    "    \n",
    "    test_x_n = out[ d ] \n",
    "    test_y_n = testdict['abnormal'][ d ]\n",
    "    \n",
    "    test_x = np.append(test_x_an, test_x_n, axis = 0)\n",
    "    test_y = np.append(test_y_an, test_y_n, axis = 0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "train_univariate_1 = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "train_univariate_1 = train_univariate_1.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_univariate_1 = tf.data.Dataset.from_tensor_slices((val_x,val_y))\n",
    "val_univariate_1 = val_univariate_1.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # create model\n",
    "        neurons = 20\n",
    "        dropout_ratio = 0.3\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(dropout_ratio))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(dropout_ratio))\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(49)\n",
    "# rand = np.random.permutation(len(out))\n",
    "# size = 115624\n",
    "# test ={}\n",
    "\n",
    "# for i in testdict.keys():\n",
    "#     testdict[i] = testdict[i][rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = create_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train, y_test =train_test_split(out[rand], testdict['abnormal'][rand],shuffle=False,test_size=.5)\n",
    "# Bad thing is can't guantree how many samples of abnormal is in x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train, y_test =train_test_split(out, testdict['abnormal'],shuffle=False,test_size=.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, \n",
    "                                                  patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bm = binary_model.fit(train_univariate_1, validation_data=val_univariate_1, epochs=300,callbacks=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bm.history['val_loss'], label='val_loss')\n",
    "plt.plot(bm.history['loss'], label='loss')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bm.history['val_accuracy'], label='val_acc')\n",
    "plt.plot(bm.history['accuracy'], label='acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = binary_model.predict(test_x)\n",
    "plt.plot(y_pred, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = binary_model.predict(test_x) > 0.5\n",
    "tf.math.confusion_matrix(test_y,y_pred , num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their does not seem to be a need to create a custom loss.\n",
    "Reason is that weihted_cross_entropy_with_logits exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startvid =0\n",
    "endvid=21\n",
    "\n",
    "testdict = Boxes(loc_files_test[startvid:endvid], box_test_txt[startvid:endvid], frames, pad ='pre', to_xywh = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out1 = lstm_20.predict(x)\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "out = np.squeeze(out)\n",
    "np.random.seed(49)\n",
    "rand = np.random.permutation(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vary_abnormal_normal_frames():\n",
    "    abnorm_index = np.where(testdict['abnormal']==1)\n",
    "    norm_index = np.where(testdict['abnormal'] == 0)\n",
    "#     rand_an = np.random.permutation(len(abnorm_index[0]))\n",
    "    abnormal_split = 0.9\n",
    "    val_split = 0.2\n",
    "    \n",
    "    \n",
    "    ratio = int(len(abnorm_index[0])*abnormal_split)\n",
    "    a = abnorm_index[0][:ratio]\n",
    "    b = norm_index[0][:ratio*weight_scale]\n",
    "    \n",
    "    initial_bias = np.log([ratio/(ratio*weight_scale)])\n",
    "\n",
    "    \n",
    "    val_ratio = int(val_split*len(a) )\n",
    "    \n",
    "    val_a = a[:val_ratio]\n",
    "    val_b = b[:val_ratio*weight_scale]\n",
    "    \n",
    "    test_a = a[val_ratio:]\n",
    "    test_b = b[val_ratio*weight_scale:]\n",
    "    \n",
    "    c = abnorm_index[0][ratio:]\n",
    "    d = norm_index[0][ratio*weight_scale:]\n",
    "    \n",
    "    index = [a,b,c,d]\n",
    "#     train  = {}\n",
    "#     test = {}\n",
    "#     for i in index[:2]:\n",
    "#         x = out[i]\n",
    "#         y = testdict['abnormal'][ i]\n",
    "#         data.append\n",
    "        \n",
    "#     for i in index[2:]:\n",
    "#         x = out[i]\n",
    "#         y = testdict['abnormal'][ i]\n",
    "#         data.append\n",
    "        \n",
    "    \n",
    "    \n",
    "    train_x_an = out[ test_a]\n",
    "    train_y_an= testdict['abnormal'][ test_a ]\n",
    "    \n",
    "    train_x_n = out[ test_b]\n",
    "    train_y_n = testdict['abnormal'][ test_b ]\n",
    "    \n",
    "    train_x = np.append(train_x_an,train_x_n,axis =0)\n",
    "    train_y = np.append(train_y_an,train_y_n, axis = 0 )\n",
    "    \n",
    "    \n",
    "    val_x_an = out[ val_a]\n",
    "    val_y_an= testdict['abnormal'][ val_a ]\n",
    "    \n",
    "    val_x_n = out[ val_b]\n",
    "    val_y_n = testdict['abnormal'][ val_b ]\n",
    "    \n",
    "    val_x = np.append(val_x_an,val_x_n,axis =0)\n",
    "    val_y = np.append(val_y_an,val_y_n, axis = 0 )\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_x_an = out[ c ]\n",
    "    test_y_an= testdict['abnormal'][ c]\n",
    "    \n",
    "    test_x_n = out[ d ] \n",
    "    test_y_n = testdict['abnormal'][ d ]\n",
    "    \n",
    "    test_x = np.append(test_x_an, test_x_n, axis = 0)\n",
    "    test_y = np.append(test_y_an, test_y_n, axis = 0)\n",
    "    \n",
    "#     return \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((val_x,val_y))\n",
    "val_univariate = val_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias = np.log([ratio/(ratio*weight_scale)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training')\n",
    "print('abnormal_mean: {}, normal_mean: {}'.format(train_x_an.mean(), train_x_n.mean()))\n",
    "print('abnormal_mean: {}, normal_mean: {}'.format(train_y_an.mean(), train_y_n.mean()))\n",
    "print(\"\\n\")\n",
    "\n",
    "print('testing')\n",
    "print('abnormal_mean: {}, normal_mean: {}'.format(test_x_an.mean(), test_x_n.mean()))\n",
    "print('abnormal_mean: {}, normal_mean: {}'.format(test_y_an.mean(), test_y_n.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary(y,x):\n",
    "    return tf.nn.weighted_cross_entropy_with_logits(y,x,pos_weight = weight_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([.8], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant([1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.nn.weighted_cross_entropy_with_logits(y,x,pos_weight = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.losses.binary_crossentropy(y, x, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_weighted(output_bias = None):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # create model\n",
    "        if output_bias is not None:\n",
    "            output_bias = keras.initializers.Constant(output_bias)\n",
    "        neurons = 30\n",
    "        dropout_ratio = 0.3\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(neurons, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(dropout_ratio))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(dropout_ratio))\n",
    "        model.add(keras.layers.Dense(1, bias_initializer=output_bias))\n",
    "        # Compile model\n",
    "#         wce = tf.nn.weighted_cross_entropy_with_logits(pos_weight = 10)\n",
    "        model.compile(loss=weighted_binary, optimizer='adam', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wl = create_baseline_weighted(output_bias = initial_bias )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0005, \n",
    "                                                  patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wl.predict(test_x).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbm = model_wl.fit(train_univariate, validation_data=val_univariate, epochs=300,callbacks = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wbm.history['loss'], '-*',label='loss')\n",
    "plt.plot(wbm.history['val_loss'], '-o',label='val_loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wbm.history['accuracy'],'-*', label='acc')\n",
    "plt.plot(wbm.history['val_accuracy'],'-*', label='val_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbm.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_wl.predict(test_x)\n",
    "plt.plot(y_pred, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_wl.predict(test_x) >0.5\n",
    "tf.math.confusion_matrix(test_y, y_pred, num_classes =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XY difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out = lstm_20.predict(x)\n",
    "# np.random.seed(49)\n",
    "# rand = np.random.permutation(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_diff = []\n",
    "for i in range(1,len(out)):\n",
    "    \n",
    "    xy_diff.append(out[i,:] - out[i-1,:])\n",
    "\n",
    "xy_diff = np.array(xy_diff)\n",
    "out =np.sum(np.square(xy_diff[:,0:2]), axis=1)\n",
    "# out =np.sum(xy_diff[:,0:4], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)  XY difference from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_index = np.where(testdict['abnormal'][1:]== 0)\n",
    "abnorm_index = np.where(testdict['abnormal'][1:]== 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(out[abnorm_index], label='abnormal frames')\n",
    "sns.distplot(out[norm_index], label='normal frames')\n",
    "plt.title('Test Video 1')\n",
    "plt.legend()\n",
    "plt.xlabel('XY Difference')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) XY difference  predicted frame from ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out = lstm_20.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_diff = []\n",
    "for i in range(0,len(out)):\n",
    "    \n",
    "    xy_diff.append(out[i,:] - y[i,:])\n",
    "\n",
    "xy_diff = np.array(xy_diff)\n",
    "out =np.sum(np.square(xy_diff[:,0:4]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_index = np.where(testdict['abnormal'][1:]== 0)\n",
    "abnorm_index = np.where(testdict['abnormal'][1:]== 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(out[abnorm_index], label='abnormal frames')\n",
    "sns.distplot(out[norm_index], label='normal frames')\n",
    "plt.title('Difference from GT Test Video 2')\n",
    "plt.legend()\n",
    "plt.xlabel('XY Difference')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # create model\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(1, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=32, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, out[rand], testdict['abnormal'][rand], cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ped[j]['frame_ppl_id'][1:,-1,0], out,'*')\n",
    "# plt.plot(ped['1']['frame_ppl_id'][1:,-1,0], xy_diff[:,0],'-+')\n",
    "# plt.plot(ped['1']['frame_ppl_id'][1:,-1,0], xy_diff[:,1],'-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped[j]['abnormal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abnormal Through IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j ='1'\n",
    "out1 = lstm_20.predict(ped[j]['x_ppl_box'])\n",
    "out = norm_train_max_min(data=out1, max1 = max1,min1 =min1,undo_norm=True)\n",
    "bb_intersection_over_union(xywh_tlbr(out), xywh_tlbr(ped[j]['y_ppl_box']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means Clustering Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out = lstm_20.predict(x)\n",
    "# out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "# out = np.squeeze(out)\n",
    "# np.random.seed(49)\n",
    "# rand = np.random.permutation(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_scal = model.predict(xx_test)\n",
    "# y_pred = norm_train_max_min(data=y_pred_scal, max1 = max1,min1 =min1,undo_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_scal = KMeans(n_clusters=2)\n",
    "kmean = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_scal.fit(y_pred_scal[:,0:2])\n",
    "kmean.fit(y_pred[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = kmean_scal.cluster_centers_\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = kmean.cluster_centers_\n",
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindict['frame_ppl_id'][0:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped = pedsort(testdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j='86'\n",
    "x,y = norm_train_max_min(data_dict=ped[j], max1=max1, min1=min1 )\n",
    "\n",
    "out1 = model.predict(x)\n",
    "bb_intersection_over_union(out1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(y_pred_scal[:,0], y_pred_scal[:,1],s=1, c='b')\n",
    "plt.scatter(out[0,0], out[0,1],s=200, c ='g', marker='s')\n",
    "plt.scatter(out[1,0], out[1,1], s=200, c ='r', marker='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred[:,0], y_pred[:,1],s=1, c='b')\n",
    "plt.scatter(out1[0,0], out1[0,1],s=200, c ='g', marker='s')\n",
    "plt.scatter(out1[1,0], out1[1,1], s=200, c ='r', marker='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestion that x,y is not enough to create a hyoer parameter to seperate abnomaly and normal pedestrains. Makes sense because abnormal and normal pedestrains walk over the same spatial features. Trying to do it for all pedestrains "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
