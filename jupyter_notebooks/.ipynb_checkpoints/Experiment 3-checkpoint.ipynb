{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of Experiment 3: Use even amount of data from from normal to abnormal and test on experiment 1 and experiment 2 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os, sys, time\n",
    "from math import floor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from collections import OrderedDict\n",
    "# from tensorflow.python.ops import math_ops\n",
    "# import tensorflow.keras.backend as kb\n",
    "from tensorflow_addons.utils.ensure_tf_install import _check_tf_version\n",
    "_check_tf_version()\n",
    "from tensorflow_addons import losses\n",
    "# Custom Functions\n",
    "# Added to path by using sys.path.append('')\n",
    "# sys.path.append('/home/akanu/git/anomalous_pred/custom_functions')\n",
    "# sys.path.insert(0,'/home/akanu/git/anomalous_pred/custom_functions')\n",
    "# sys.path.append('/mnt/roahm/users/akanu/git/anomalous_pred/custom_functions')\n",
    "from load_data import Files_Load, Boxes, test_split_norm_abnorm, norm_train_max_min\n",
    "from custom_metrics import bb_intersection_over_union, bb_intersection_over_union_np\n",
    "from coordinate_change import xywh_tlbr, tlbr_xywh\n",
    "from pedsort import pedsort\n",
    "from load_data_binary_class import return_indices, binary_data_split, same_ratio_split_train_val\n",
    "from load_data_binary_class import one_weight_ratio_train\n",
    "from TP_TN_FP_FN import seperate_misclassifed_examples, sort_TP_TN_FP_FN_by_vid_n_frame\n",
    "# from plot_all_videos import cycle_through_videos\n",
    "\n",
    "# Kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Need to figue out why this is not working\n",
    "# from persistence1d import RunPersistence\n",
    "\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = 20\n",
    "\n",
    "train_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Train_Box/\"\n",
    "test_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Test_Box/\"\n",
    "\n",
    "loc_files_train, loc_files_test, box_train_txt, box_test_txt = Files_Load(train_file, test_file)\n",
    "\n",
    "# Don't forget to change to xywh \n",
    "traindict = Boxes(loc_files_train, box_train_txt, frames, pad ='pre', to_xywh=True)\n",
    "testdict = Boxes(loc_files_test, box_test_txt, frames, pad ='pre', to_xywh = True)\n",
    "abnormal_dict, normal_dict = test_split_norm_abnorm(testdict) #splits by which predicted frames are normal or not\n",
    "\n",
    "# Normilize data\n",
    "max1 = traindict['x_ppl_box'].max()\n",
    "min1 = traindict['x_ppl_box'].min()\n",
    "xx,yy = norm_train_max_min(data_dict = traindict, max1=max1,min1=min1)\n",
    "xx_norm,yy_norm = norm_train_max_min(data_dict = normal_dict, max1=max1,min1=min1)\n",
    "xx_abnorm,yy_abnorm = norm_train_max_min(data_dict = abnormal_dict, max1=max1,min1=min1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "xx_train, xx_val,yy_train,yy_val = train_test_split(xx,yy, test_size = 0.3)\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((xx_train,yy_train))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((xx_val,yy_val))\n",
    "val_univariate = val_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load or Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to figure out a better way to add more Python Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/home/akanu/git/anomalous_pred/model')\n",
    "from lstm_models import lstm_xywh_avenue_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loc = '/home/akanu/git/anomalous_pred/saved_models'\n",
    "nc=['lstm', 'xywh', 'avenue', frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = os.path.join(model_loc,\n",
    "                            '{}_{}_{}_{}.h5'.format(nc[0], nc[1],nc[2], nc[3]))\n",
    "lstm_20 = tf.keras.models.load_model(loaded_model,  \n",
    "                                   custom_objects = {'loss':'mse'} , \n",
    "                                   compile=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight ratio for inital bias and ratio for weighted binary\n",
    "# might be different thats why error could be occuring\n",
    "# initial_bias = np.log([pos/neg])\n",
    "weight_ratio =1\n",
    "initial_bias = np.log([weight_ratio]) # want to take out of here\n",
    "initial_bias\n",
    "# Note that value is negative now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels * -log(sigmoid(logits)) * pos_weight +\n",
    "    (1 - labels) * -log(1 - sigmoid(logits)) \n",
    "    \n",
    "This means that the into must be 1/weight ratio. Assuming\n",
    "weight_ratio = pos/neg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_binary(y,x):\n",
    "#     return tf.nn.weighted_cross_entropy_with_logits(y,x,pos_weight = 1/weight_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    when_y_1 = y_true*tf.keras.backend.log(y_pred)*(1/weight_ratio)\n",
    "    neg_y_pred = Lambda(lambda x: -x)(y_pred)\n",
    "    when_y_0 = ( 1+Lambda(lambda x: -x)(y_true))*tf.keras.backend.log(1+neg_y_pred )\n",
    "    \n",
    "    weighted_cross_entr = Lambda(lambda x: -x)(when_y_0+when_y_1)\n",
    "    return weighted_cross_entr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data for Binary Network Experiment 1\n",
    "## Seed was 24\n",
    "## Then I used 81\n",
    "## Then 23\n",
    "# Then 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_exp_1 = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_exp_1 = return_indices(testdict['abnormal'], \n",
    "                               abnormal_split = 0.5, \n",
    "                               seed=seed_exp_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_exp_1,y_exp_1 = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115624, 20, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_exp_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COuld probably make binary data split easier for adding more features\n",
    "by performing data auc outside and inputing that into function. Might look into as well to make code more maintainable. Could allow me to call the function in some ratio split train val function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_x_exp_1,train_full_y_exp_1, test_x_exp_1, test_y_exp_1 = binary_data_split(x_exp_1,\n",
    "                                                                          y_exp_1,\n",
    "                                                                          lstm_20,\n",
    "                                                                          indices_exp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing shape length of train_full_x and test_x \n",
      "train_full_x: (2, 112694), test_x:(2, 2930)\n"
     ]
    }
   ],
   "source": [
    "print('Comparing shape length of train_full_x and test_x \\n' +\n",
    "     'train_full_x: {}, test_x:{}'.format(train_full_x_exp_1.shape,\n",
    "                                         test_x_exp_1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_even_split_exp_1, train_y_even_split_exp_1 = one_weight_ratio_train(\n",
    "                                        train_full_x_exp_1,\n",
    "                                        train_full_y_exp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x_exp_1, val_y_exp_1, train_x_exp_1, train_y_exp_1 = same_ratio_split_train_val(\n",
    "    train_x_even_split_exp_1,train_y_even_split_exp_1, val_ratio = 0.3)\n",
    "# weight_ratio = len(indices_exp_1[0])/len(indices_exp_1[1])\n",
    "weight_ratio =1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "# Note that I need to index into correct column as one is IOU\n",
    "# and last column is index values to map back into datadicg\n",
    "train_bm_exp_1 = tf.data.Dataset.from_tensor_slices((train_x_exp_1[0,:],train_y_exp_1))\n",
    "train_bm_exp_1 = train_bm_exp_1.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_bm_exp_1 = tf.data.Dataset.from_tensor_slices((val_x_exp_1[0,:],val_y_exp_1))\n",
    "val_bm_exp_1 = val_bm_exp_1.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/home/akanu/git/anomalous_pred/model')\n",
    "from binary_classification import Dense_5_Drop_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_bin_exp_1 = ['seed{}_Dense_5_Drop_5_exp_1', 'xywh','avenue', frames, 0.500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_bin_exp_1[0] = nc_bin_exp_1[0].format(seed_exp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seed23_Dense_5_Drop_5_exp_1', 'xywh', 'avenue', 20, 0.5]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc_bin_exp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/akanu/git/anomalous_pred/saved_models/Experiment_3'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loc_exp = os.path.join(model_loc, 'Experiment_3')\n",
    "model_loc_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data Experiment 1 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# binary_model = create_baseline_weighted(initial_bias)\n",
    "bm_history_exp_1,bm_model_exp_1 = Dense_5_Drop_2(train_bm_exp_1,\n",
    "                              val_bm_exp_1,\n",
    "                              os.path.join(model_loc_exp, \n",
    "                                            'using_exp_1_method'),\n",
    "                              nc_bin_exp_1,\n",
    "                              loss,\n",
    "                              initial_bias,\n",
    "                              1000,\n",
    "                              save_model=False,\n",
    "                              patience = 50\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and Save Results using experiment\n",
    "# 1 methodogly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seed23_Dense_5_Drop_5_exp_1', 'xywh', 'avenue', 20, 0.5]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is location where  plots are saved\n",
    "plot_loc = '/home/akanu/git/anomalous_pred/plots/experiment_3/Avenue'\n",
    "# nc_bin provides unique idenifer for plots\n",
    "nc_bin_exp_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/akanu/git/anomalous_pred/plots/experiment_3/Avenue/using_exp_1_method'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_loc_exp_1 = os.path.join(plot_loc, 'using_exp_1_method')\n",
    "plot_loc_exp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( nrows=1, ncols=1 )\n",
    "ax.plot(bm_history_exp_1.history['loss'], '-', color='black',label='loss')\n",
    "ax.plot(bm_history_exp_1.history['val_loss'],'-',color='red' ,label ='Validation loss')\n",
    "ax.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Zero Initial Bias Last Layer')\n",
    "# fig.savefig(os.path.join(plot_loc_exp_1, 'loss_{}_{}_{}_{}_{}.jpg'.format(nc_bin_exp_1[0],\n",
    "#                                                                     nc_bin_exp_1[1],\n",
    "#                                                                     nc_bin_exp_1[2],\n",
    "#                                                                     nc_bin_exp_1[3],\n",
    "#                                                                     nc_bin_exp_1[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( nrows=1, ncols=1 )\n",
    "ax.plot(bm_history_exp_1.history['accuracy'], '-*', color='black',label='Acc')\n",
    "ax.plot(bm_history_exp_1.history['val_accuracy'],'-o', color='red', label ='Validation Acc')\n",
    "ax.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Zero Initial Bias Last Layer')\n",
    "# fig.savefig(os.path.join(plot_loc_exp_1, 'acc_{}_{}_{}_{}_{}.jpg'.format(nc_bin_exp_1[0],\n",
    "#                                                                     nc_bin_exp_1[1],\n",
    "#                                                                     nc_bin_exp_1[2],\n",
    "#                                                                     nc_bin_exp_1[3],\n",
    "#                                                                     nc_bin_exp_1[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(bm_model_exp_1.predict(test_x_exp_1[0,:]), '*')\n",
    "plt.title('Zero Initial Bias Last Layer')\n",
    "plt.ylabel('Predicted Y')\n",
    "# fig.savefig(os.path.join(plot_loc_exp_1, 'scatter_{}_{}_{}_{}_{}.jpg'.format(nc_bin_exp_1[0],\n",
    "#                                                                     nc_bin_exp_1[1],\n",
    "#                                                                     nc_bin_exp_1[2],\n",
    "#                                                                     nc_bin_exp_1[3],\n",
    "#                                                                     nc_bin_exp_1[4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Binary Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/akanu/git/anomalous_pred/saved_models/Experiment_3/using_exp_1_method/seed23_Dense_5_Drop_5_exp_1_xywh_avenue_20_0.500.h5'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_binary_model = os.path.join(model_loc_exp,'using_exp_1_method')\n",
    "load_binary_model = os.path.join(load_binary_model, \n",
    "                     '{}_{}_{}_{}_{:.3f}.h5'.format(nc_bin_exp_1[0],\n",
    "                                                   nc_bin_exp_1[1], \n",
    "                                                   nc_bin_exp_1[2],\n",
    "                                                   nc_bin_exp_1[3],\n",
    "                                                   nc_bin_exp_1[4] ) )\n",
    "load_binary_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_binary_model = tf.keras.models.load_model(load_binary_model,  \n",
    "                                   custom_objects = {'loss': loss},\n",
    "                                   compile = True)\n",
    "bm_model_exp_1 = loaded_binary_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_exp_1 = bm_model_exp_1.predict(test_x_exp_1[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1055,  410],\n",
       "       [ 231, 1234]], dtype=int32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_exp_1 = bm_model_exp_1.predict(test_x_exp_1[0,:]) > 0.5\n",
    "conf_matrix_exp_1 = tf.math.confusion_matrix(test_y_exp_1, y_pred_exp_1)\n",
    "conf_matrix_exp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_index_exp_1 = seperate_misclassifed_examples(bm_model_exp_1,\n",
    "                                                       test_x_exp_1,\n",
    "                                                       test_y_exp_1,\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01851344108581543\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "s_TP_TN_FP_FN,s_boxes_dict = sort_TP_TN_FP_FN_by_vid_n_frame(testdict, confusion_index_exp_1 )\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indi_seq(data, video, frame):\n",
    "    \"\"\"\n",
    "    Takes the data and find the video and frame so that \n",
    "    sequence can be inputted and creates smaller dict of one \n",
    "    sequence\n",
    "\n",
    "    data: Format is keys are different videos\n",
    "    video: video of interest. useful if in TN, TP, FN, FP. Ny that\n",
    "            I mean that only inputed then the next immediate key is\n",
    "            video numbers. For generic data this wouldn't work.\n",
    "            As I would need to find the videos. Probably go back and \n",
    "            delete this funciton later on\n",
    "    return output: same format as x_ppl_box,y_ppl_box etx\n",
    "    \"\"\"\n",
    "\n",
    "    output ={}\n",
    "    data = data['{}.txt'.format(video)]\n",
    "    frames = data['frame_ppl_id'][:,-1,0]\n",
    "    found_index_of_frame = np.where(frames==frame)\n",
    "\n",
    "    \n",
    "    for key in data.keys():\n",
    "\n",
    "        output[key] = data[key][found_index_of_frame]\n",
    "\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_sequence(one_ped_seq, max1, min1, vid_key,pic_loc, loc_videos, xywh=False):\n",
    "    \"\"\"\n",
    "    This will plot the sequences of the of one pedestrain\n",
    "    Not computially efficent if you want to plot lots of pedestrain\n",
    "\n",
    "\n",
    "    one_ped_seq: one pedestrain sequence: 'x_ppl_box', 'y_ppl_box',\n",
    "                     'frame_ppl_id', 'video_file', 'abnormal'. From ind_seq\n",
    "                     function\n",
    "    max1:  scaling factor\n",
    "    min1:  scaline factor\n",
    "    vid_key: '01.txt' '01_02.txt'etx\n",
    "    loc_videos: this are videos that are used to make plots\n",
    "    pic_loc: need to save to different location depending on confusion\n",
    "                    type and/or can input generic location to allow for\n",
    "                    plotting all videos at the same sort_TP_TN_FP_FN_by_vid_n_frame                \n",
    "    \"\"\"\n",
    "\n",
    "    x_input = one_ped_seq['x_ppl_box'].squeeze()\n",
    "    # x_scal,y_scal = norm_train_max_min(data_dict = data, max1 = max1,min1 =min1)\n",
    "    last_frame = one_ped_seq['frame_ppl_id'][-1,-2,0]\n",
    "    size = len(x_input)\n",
    "    \n",
    "\n",
    "    next_frame_index, j, frame_count = 0, 0, 0\n",
    "\n",
    "    loc_vid = os.path.join(loc_videos, vid_key[:-4]+ '.avi')\n",
    "    video_capture = cv2.VideoCapture(loc_vid)\n",
    "\n",
    "    # there could be information lost here\n",
    "    if xywh:\n",
    "        x_input[:,0]  =  x_input[:,0]  -  x_input[:,2]/2\n",
    "        x_input[:,1]  =  x_input[:,1]  -  x_input[:,3]/2 # Now we are at tlwh\n",
    "        x_input[:,2:] =  x_input[:,:2] +  x_input[:,2:]\n",
    "\n",
    "\n",
    "#     x_input = x_input.squeeze()\n",
    "    frame_ppl = one_ped_seq['frame_ppl_id'].squeeze()\n",
    "\n",
    "\n",
    "    for i in range(0, last_frame+1):\n",
    "        ret, frame = video_capture.read()\n",
    "        if i == frame_ppl[j,0 ]: #finds the frames\n",
    "            while i == frame_ppl[j,0]:\n",
    "\n",
    "                x_box = x_input[j]\n",
    "                id1 = frame_ppl[j,1]\n",
    "\n",
    "                input_frame = frame.copy()\n",
    "                # Since camera is statiornay I can plot other bbox as well on same video\n",
    "                # Input Data\n",
    "                cv2.rectangle(input_frame, (int(x_box[0]), int(x_box[1])), (int(x_box[2]), int(x_box[3])),(0,255,0), 2)\n",
    "                # Need to change This\n",
    "                vid_str_info = vid_key[:-4] + '___' + str(i) + '__' + str(id1)\n",
    "                cv2.imwrite( os.path.join(pic_loc, vid_str_info + '_input.jpg'), input_frame)\n",
    "                \n",
    "                frame_count += 1\n",
    "                next_frame_index += 1\n",
    "                j = next_frame_index\n",
    "                \n",
    "                if j == size:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid= '/home/akanu/Dataset/Anomaly/Avenue_Dataset/testing_videos'\n",
    "# conf_types = ['FN', 'FP', 'TN', 'TP']\n",
    "# for conf_type in conf_types:\n",
    "#     pic_loc = '/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_xywh/confusion_martix/using_exp_1/{}/{}'.format(seed_exp_1, conf_type)\n",
    "#     cycle_through_videos(lstm_20, s_boxes_dict[conf_type], max1, min1, pic_loc=pic_loc, loc_videos=vid,xywh= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_of_interest = indi_seq(s_boxes_dict['FN'],'09' , 495)\n",
    "\n",
    "                      \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid= '/home/akanu/Dataset/Anomaly/Avenue_Dataset/testing_videos'\n",
    "\n",
    "pic_loc = '/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_xywh/confusion_martix/using_exp_1/{}/ind_seq'.format(seed_exp_1)\n",
    "plot_sequence(seq_of_interest, \n",
    "              max1,\n",
    "              min1,\n",
    "              '09.txt',\n",
    "              pic_loc,\n",
    "              loc_videos = vid, \n",
    "              xywh=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFUSION_COUNT = np.zeros((4,1))\n",
    "for con_key,i in zip(s_TP_TN_FP_FN.keys(), np.arange(0,4)):\n",
    "    for vid_key in s_TP_TN_FP_FN[con_key].keys():\n",
    "        CONFUSION_COUNT[i] += len(s_TP_TN_FP_FN[con_key][vid_key])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFUSION_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loc_exp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv_exp_1 = '/home/akanu/git/anomalous_pred/plots/experiment_3/Avenue/using_exp_1_method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv_exp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(conf_matrix_exp_1.numpy()).to_csv(os.path.join(save_csv_exp_1, \n",
    "                    'conf_matrix_{}_{}_{}_{}_{}.csv'.format(\n",
    "                                                nc_bin_exp_1[0],\n",
    "                                                nc_bin_exp_1[1],\n",
    "                                                nc_bin_exp_1[2],\n",
    "                                                nc_bin_exp_1[3],\n",
    "                                                nc_bin_exp_1[4])),\n",
    "                                                index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_exp_1, tpr_exp_1, _exp_1 = roc_curve(test_y_exp_1, \n",
    "                          bm_model_exp_1.predict(test_x_exp_1[0,:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_exp_1, recall_exp_1, threshold_exp_1 = precision_recall_curve(\n",
    "                        test_y_exp_1,\n",
    "                        bm_model_exp_1.predict(test_x_exp_1[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tpr_exp_1 = pd.DataFrame({'fpr':fpr_exp_1, \n",
    "                              'tpr': tpr_exp_1,\n",
    "                              '_exp_1':_exp_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_recall_thre_exp_1 = pd.DataFrame({'precision': prec_exp_1,\n",
    "                                      'recall': recall_exp_1})\n",
    "prec_recall_thre_exp_1['threshold']= pd.Series(threshold_exp_1)\n",
    "# did this because their was a missing index value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tpr_exp_1.to_csv(os.path.join(save_csv_exp_1, \n",
    "                    'fpr_tpr_exp_1_{}_{}_{}_{}_{}.csv'.format(\n",
    "                                                nc_bin_exp_1[0],\n",
    "                                                nc_bin_exp_1[1],\n",
    "                                                nc_bin_exp_1[2],\n",
    "                                                nc_bin_exp_1[3],\n",
    "                                                nc_bin_exp_1[4])),\n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_recall_thre_exp_1.to_csv(os.path.join(save_csv_exp_1, \n",
    "                    'prec_recall_thre_exp_1_{}_{}_{}_{}_{}.csv'.format(\n",
    "                                                nc_bin_exp_1[0],\n",
    "                                                nc_bin_exp_1[1],\n",
    "                                                nc_bin_exp_1[2],\n",
    "                                                nc_bin_exp_1[3],\n",
    "                                                nc_bin_exp_1[4])),\n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(save_csv_exp_1, \n",
    "                    'fpr_tpr_exp_1_{}_{}_{}_{}_{}.csv'.format(\n",
    "                                                nc_bin_exp_1[0],\n",
    "                                                nc_bin_exp_1[1],\n",
    "                                                nc_bin_exp_1[2],\n",
    "                                                nc_bin_exp_1[3],\n",
    "                                                nc_bin_exp_1[4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from experiment 2\n",
    "## Seed is 81\n",
    "## Seed is 24\n",
    "## Seed is 23\n",
    "## Seed is 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_exp_2 =35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_exp_2 = return_indices(testdict['abnormal'], \n",
    "                               abnormal_split = 0.5,\n",
    "                               seed=seed_exp_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_exp_2,y_exp_2 = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_per_combine,train_y_per_combine, test_x_exp_2, test_y_exp_2 = binary_data_split(x_exp_2,\n",
    "                                                                          y_exp_2,\n",
    "                                                                          lstm_20,\n",
    "                                                                          indices_exp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = lstm_20.predict(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bb_intersection_over_union_np(xywh_tlbr(out),xywh_tlbr(yy))\n",
    "out = np.squeeze(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE THAT I DID NOT SAVE INDEX Values for TRAINING DATA\n",
    "# I JUST SET VALUE TO BE None. THis means that the training data from\n",
    "# LSTM can be filitered using None\n",
    "training_set_from_lstm = np.append(np.reshape(out,(1, len(out)) ), \n",
    "                                   np.array([None]*len(out)).reshape((1,len(out))),\n",
    "                                   axis=0)\n",
    "train_x_with_lstm_train_set = np.append(train_x_per_combine,training_set_from_lstm, axis=1)\n",
    "train_y_with_lstm_train_set = np.append(train_y_per_combine,\n",
    "                                        np.zeros(len(out))) # zeros cuz normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_even_split_exp_2, train_y_even_split_exp_2 = one_weight_ratio_train(\n",
    "    train_x_with_lstm_train_set, train_y_with_lstm_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x_exp_2, val_y_exp_2, train_x_exp_2, train_y_exp_2 = same_ratio_split_train_val(\n",
    "    train_x_even_split_exp_2,train_y_even_split_exp_2, val_ratio = 0.3)\n",
    "# weight_ratio = len(indices_exp_1[0])/len(indices_exp_1[1])\n",
    "weight_ratio =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "# Note that I need to index into correct row as one is IOU\n",
    "# and last row is index values to map back into datadicg\n",
    "# Needed to type cast it because when I put None for index values\n",
    "# Array turned into an object dtype\n",
    "train_bm_exp_2 = tf.data.Dataset.from_tensor_slices((train_x_exp_2[0,:].astype('float64'),\n",
    "                                                train_y_exp_2))\n",
    "train_bm_exp_2 = train_bm_exp_2.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_bm_exp_2 = tf.data.Dataset.from_tensor_slices((val_x_exp_2[0,:].astype('float64'),\n",
    "                                             val_y_exp_2))\n",
    "val_bm_exp_2 = val_bm_exp_2.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Comparing shape length of train_full_x and test_x \\n' +\n",
    "#      'train_full_x: {}, test_x:{}'.format(train_x_with_lstm_train_set.shape,\n",
    "#                                          test_x_exp_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_bin_exp_2 = ['seed{}_Dense_5_Drop_5_exp_2', 'xywh','avenue', frames, 0.500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_bin_exp_2[0] = nc_bin_exp_2[0].format(seed_exp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_bin_exp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loc_exp = os.path.join(model_loc, 'Experiment_3')\n",
    "model_loc_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data Experiment 2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_model = create_baseline_weighted(initial_bias)\n",
    "bm_history_exp_2,bm_model_exp_2 = Dense_5_Drop_2(train_bm_exp_2,\n",
    "                              val_bm_exp_2,\n",
    "                              os.path.join(model_loc_exp, \n",
    "                                           'using_exp_2_method'),\n",
    "                              nc_bin_exp_2,\n",
    "                              loss,\n",
    "                              initial_bias,\n",
    "                              1000,\n",
    "                              save_model=False,\n",
    "                              patience = 50                                     \n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and save results of experiment 2 \n",
    "# methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is location where  plots are saved\n",
    "plot_loc = '/home/akanu/git/anomalous_pred/plots/experiment_3/Avenue'\n",
    "# nc_bin provides unique idenifer for plots\n",
    "nc_bin_exp_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loc_exp_2 = os.path.join(plot_loc, 'using_exp_2_method')\n",
    "plot_loc_exp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( nrows=1, ncols=1 )\n",
    "ax.plot(bm_history_exp_2.history['loss'], '-', color='black',label='loss')\n",
    "ax.plot(bm_history_exp_2.history['val_loss'],'-',color='red' ,label ='Validation loss')\n",
    "ax.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Zero Initial Bias Last Layer')\n",
    "# fig.savefig(os.path.join(plot_loc_exp_2, 'loss_{}_{}_{}_{}_{}.jpg'.format(nc_bin_exp_2[0],\n",
    "#                                                                     nc_bin_exp_2[1],\n",
    "#                                                                     nc_bin_exp_2[2],\n",
    "#                                                                     nc_bin_exp_2[3],\n",
    "#                                                                     nc_bin_exp_2[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( nrows=1, ncols=1 )\n",
    "ax.plot(bm_history_exp_2.history['accuracy'], '-*', color='black',label='Acc')\n",
    "ax.plot(bm_history_exp_2.history['val_accuracy'],'-o', color='red', label ='Validation Acc')\n",
    "ax.legend(loc=4)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Zero Initial Bias Last Layer')\n",
    "# fig.savefig(os.path.join(plot_loc_exp_2, 'acc_{}_{}_{}_{}_{}.jpg'.format(nc_bin_exp_2[0],\n",
    "#                                                                     nc_bin_exp_2[1],\n",
    "#                                                                     nc_bin_exp_2[2],\n",
    "#                                                                     nc_bin_exp_2[3],\n",
    "#                                                                     nc_bin_exp_2[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(bm_model_exp_2.predict(test_x_exp_1[0,:]), '*')\n",
    "plt.title('Zero Initial Bias Last Layer')\n",
    "plt.ylabel('Predicted Y')\n",
    "# fig.savefig(os.path.join(plot_loc_exp_2, 'scatter_{}_{}_{}_{}_{}.jpg'.format(nc_bin_exp_2[0],\n",
    "#                                                                     nc_bin_exp_2[1],\n",
    "#                                                                     nc_bin_exp_2[2],\n",
    "#                                                                     nc_bin_exp_2[3],\n",
    "#                                                                     nc_bin_exp_2[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_exp_2 = bm_model_exp_2.predict(test_x_exp_2[0,:]) > 0.5\n",
    "conf_matrix_exp_2 = tf.math.confusion_matrix(test_y_exp_2, y_pred_exp_2)\n",
    "conf_matrix_exp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(conf_matrix_exp_2.numpy()).to_csv(os.path.join(plot_loc_exp_2, \n",
    "                    'conf_matrix_{}_{}_{}_{}_{}.csv'.format(\n",
    "                                                nc_bin_exp_2[0],\n",
    "                                                nc_bin_exp_2[1],\n",
    "                                                nc_bin_exp_2[2],\n",
    "                                                nc_bin_exp_2[3],\n",
    "                                                nc_bin_exp_2[4])),\n",
    "                                                index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaded Binary Model using \n",
    "# experiment 2 method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_binary_model_2 = os.path.join(model_loc_exp,'using_exp_2_method')\n",
    "load_binary_model_2 = os.path.join(load_binary_model_2, \n",
    "                     '{}_{}_{}_{}_{:.3f}.h5'.format(nc_bin_exp_2[0],\n",
    "                                                   nc_bin_exp_2[1], \n",
    "                                                   nc_bin_exp_2[2],\n",
    "                                                   nc_bin_exp_2[3],\n",
    "                                                   nc_bin_exp_2[4] ) )\n",
    "load_binary_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_binary_model_2 = tf.keras.models.load_model(load_binary_model_2,  \n",
    "                                   custom_objects = {'loss': loss},\n",
    "                                    compile = True)\n",
    "bm_model_exp_2 = loaded_binary_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_exp_2 = bm_model_exp_2.predict(test_x_exp_2[0,:]) > 0.5\n",
    "conf_matrix_exp_2 = tf.math.confusion_matrix(test_y_exp_2, y_pred_exp_2)\n",
    "conf_matrix_exp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_index_exp_2 = seperate_misclassifed_examples(bm_model_exp_2,\n",
    "                                                       test_x_exp_2,\n",
    "                                                       test_y_exp_2,\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "s_TP_TN_FP_FN_exp_2,s_boxes_dict_exp_2 = sort_TP_TN_FP_FN_by_vid_n_frame(testdict, confusion_index_exp_2 )\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid= '/home/akanu/Dataset/Anomaly/Avenue_Dataset/testing_videos'\n",
    "conf_types = ['FN', 'FP', 'TN', 'TP']\n",
    "for conf_type in conf_types:\n",
    "    pic_loc = '/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_xywh/confusion_martix/using_exp_2/{}/{}'.format(seed_exp_2, conf_type)\n",
    "    cycle_through_videos(lstm_20, s_boxes_dict_exp_2[conf_type], max1, min1, pic_loc=pic_loc, loc_videos=vid,xywh= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_exp_2, tpr_exp_2, _exp_2 = roc_curve(test_y_exp_2, \n",
    "                          bm_model_exp_2.predict(test_x_exp_2[0,:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_exp_2, tpr_exp_2, _exp_2 = roc_curve(test_y_exp_2, \n",
    "                              bm_model_exp_2.predict(test_x_exp_2[0,:]), \n",
    "                            sample_weight=np.linspace(0,1,2930))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_exp_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_exp_2, recall_exp_2, threshold_exp_2 = precision_recall_curve(\n",
    "                        test_y_exp_2,\n",
    "                        bm_model_exp_2.predict(test_x_exp_2[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tpr_exp_2 = pd.DataFrame({'fpr':fpr_exp_2, \n",
    "                              'tpr': tpr_exp_2,\n",
    "                              '_exp_2':_exp_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_recall_thre_exp_2 = pd.DataFrame({'precision': prec_exp_2,\n",
    "                                      'recall': recall_exp_2})\n",
    "prec_recall_thre_exp_2['threshold']= pd.Series(threshold_exp_2)\n",
    "# did this because their was a missing index value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tpr_exp_2.to_csv(os.path.join(plot_loc_exp_2, \n",
    "                    'fpr_tpr_exp_2_{}_{}_{}_{}_{}.csv'.format(\n",
    "                                                nc_bin_exp_2[0],\n",
    "                                                nc_bin_exp_2[1],\n",
    "                                                nc_bin_exp_2[2],\n",
    "                                                nc_bin_exp_2[3],\n",
    "                                                nc_bin_exp_2[4])),\n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_recall_thre_exp_2.to_csv(os.path.join(plot_loc_exp_2, \n",
    "                    'prec_recall_thre_exp_2_{}_{}_{}_{}_{}.csv'.format(\n",
    "                                                nc_bin_exp_2[0],\n",
    "                                                nc_bin_exp_2[1],\n",
    "                                                nc_bin_exp_2[2],\n",
    "                                                nc_bin_exp_2[3],\n",
    "                                                nc_bin_exp_2[4])),\n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
