{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os, sys, time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from collections import OrderedDict\n",
    "# from tensorflow.python.ops import math_ops\n",
    "# import tensorflow.keras.backend as kb\n",
    "from tensorflow_addons.utils.ensure_tf_install import _check_tf_version\n",
    "_check_tf_version()\n",
    "from tensorflow_addons import losses\n",
    "\n",
    "# Custom Functions\n",
    "# Added to path by using sys.path.append('')\n",
    "sys.path.append('/home/akanu/git/anomalous_pred/custom_functions')\n",
    "from load_data import Files_Load, Boxes, test_split_norm_abnorm, norm_train_max_min\n",
    "from custom_metrics import bb_intersection_over_union, bb_intersection_over_union_np\n",
    "from coordinate_change import xywh_tlbr, tlbr_xywh\n",
    "from pedsort import pedsort\n",
    "\n",
    "# Kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Need to figue out why this is not working\n",
    "# from persistence1d import RunPersistence\n",
    "\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "#   try:\n",
    "#     tf.config.experimental.set_virtual_device_configuration(\n",
    "#         gpus[1],\n",
    "#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5500)])\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Virtual devices must be set before GPUs have been initialized\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = 20\n",
    "# startvid=0\n",
    "# endvid=21\n",
    "\n",
    "train_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Train_Box/\"\n",
    "test_file = \"/home/akanu/Dataset/Anomaly/Avenue_Dataset/bounding_box_tlbr/Txt_Data/Test_Box/\"\n",
    "\n",
    "loc_files_train, loc_files_test, box_train_txt, box_test_txt = Files_Load(train_file, test_file)\n",
    "\n",
    "# Don't forget to change to xywh \n",
    "traindict = Boxes(loc_files_train, box_train_txt, frames, pad ='pre', to_xywh=True)\n",
    "testdict = Boxes(loc_files_test, box_test_txt, frames, pad ='pre', to_xywh = True)\n",
    "abnormal_dict, normal_dict = test_split_norm_abnorm(testdict) #splits by which predicted frames are normal or not\n",
    "\n",
    "# Normilize data\n",
    "max1 = traindict['x_ppl_box'].max()\n",
    "min1 = traindict['x_ppl_box'].min()\n",
    "xx,yy = norm_train_max_min(data_dict = traindict, max1=max1,min1=min1)\n",
    "xx_norm,yy_norm = norm_train_max_min(data_dict = normal_dict, max1=max1,min1=min1)\n",
    "xx_abnorm,yy_abnorm = norm_train_max_min(data_dict = abnormal_dict, max1=max1,min1=min1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "xx_train, xx_val,yy_train,yy_val = train_test_split(xx,yy, test_size = 0.3)\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((xx_train,yy_train))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((xx_val,yy_val))\n",
    "val_univariate = val_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/opt/ros/melodic/lib/python2.7/dist-packages', '/home/akanu/.virtualenvs/anomalous_pred/lib/python36.zip', '/home/akanu/.virtualenvs/anomalous_pred/lib/python3.6', '/home/akanu/.virtualenvs/anomalous_pred/lib/python3.6/lib-dynload', '/usr/lib/python3.6', '/home/akanu/.virtualenvs/anomalous_pred/lib/python3.6/site-packages', '/home/akanu/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/IPython/extensions', '/tmp/tmpp44q_9ss', '/home/akanu/git/anomalous_pred/custom_functions']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sys.path.insert(0,'/home/akanu/git/anomalous_pred/model')\n",
    "# from lstm_models import lstm_xywh_avenue_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loc = '/home/akanu/git/anomalous_pred/saved_models'\n",
    "nc=['lstm', 'xywh', 'avenue', frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lstm_20 = lstm_xywh_avenue_20(train_data = train_univariate,\n",
    "#                               val_data = val_univariate,\n",
    "#                               model_loc = model_loc,\n",
    "#                               nc=nc,\n",
    "#                               epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_20' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-eeeeb0e8d5cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-o'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/akanu/git/anomalous_pred/model/plots_models'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'loss_{}_{}_{}_{}.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm_20' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots( nrows=1, ncols=1 )\n",
    "ax.plot(lstm_20.history['val_loss'],'-o',label='Validation Set')\n",
    "ax.plot(lstm_20.history['loss'], '-*', label='Train Set')\n",
    "image_loc = '/home/akanu/git/anomalous_pred/model/plots_models'\n",
    "image = 'loss_{}_{}_{}_{}.jpg'\n",
    "ax.legend()\n",
    "plt.title('Loss')\n",
    "# plt.savefig(os.path.join(image_loc,image.format(nc[0], nc[1],nc[2], nc[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model (If model is trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = os.path.join(model_loc,'{}_{}_{}_{}.h5'.format(nc[0], nc[1],nc[2], nc[3]))\n",
    "model = tf.keras.models.load_model(loaded_model,  \n",
    "                                   custom_objects = {'loss':'mse'} , \n",
    "                                   compile=True)\n",
    "lstm_20 = model\n",
    "# Appears because of my custom loss fuction I could use evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic_loc = \"/home/akanu/git/anomalous_pred/resulting_images/xywh/test_abnorm/{}_{}_{}\"\n",
    "# visual_ouput(model=lstm_20,max1 = max1, min1=min1, vid=1,pic_loc =pic_loc, output_dict=abnormal_dict,xywh = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic_loc = \"/home/akanu/git/anomalous_pred/resulting_images/xywh/test_norm/{}_{}_{}\"\n",
    "# visual_ouput(model=lstm_20,max1 = max1, min1=min1, vid=1,pic_loc =pic_loc, output_dict=normal_dict,xywh = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats going on right here is that you are loading all the dataset at once.\n",
    "As a result when you say pedsort['1'] its not as meaning one person who is at in\n",
    "one video but person one from all the videos in that dataset.\n",
    "(when you load all the videos at the same time).\n",
    "To fix it would just need to for loop it and save dict to another dict\n",
    "Would have a unique dict set for each video, then another dict set containing \n",
    "each person. Then inside that another dict set with the unful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j ='1'\n",
    "ped = pedsort(testdict)\n",
    "x,y = norm_train_max_min(data_dict=ped[j], max1 = max1, min1=min1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = lstm_20.predict(x)\n",
    "\n",
    "\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1), xywh_tlbr(y))\n",
    "plt.plot(ped[j]['frame_ppl_id'][:,-1,0], np.squeeze(out),'-')\n",
    "plt.plot(ped[j]['frame_ppl_id'][:,-1,0], ped[j]['abnormal'])\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('IOU')\n",
    "print(np.mean(out), out.shape)\n",
    "# sns.distplot(np.squeeze(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using averaged iou to find Abnormal vs normal\n",
    "## Doesn't work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tries to find normal/abnormal pedestrian from averaged iou\n",
    "iou_sum = []\n",
    "for j in ped.keys():\n",
    "#     print(j)\n",
    "    x,y = norm_train_max_min(data_dict=ped[j], max1 = max1, min1=min1 )\n",
    "    out1 = lstm_20.predict(x)\n",
    "    out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "#     print(out.shape)\n",
    "    iou_sum.append([np.mean(out), np.any(ped[j]['abnormal']) ])\n",
    "iou_sum = np.array(iou_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_ped = iou_sum[np.nonzero(iou_sum[:,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ped = iou_sum[np.where(iou_sum[:,1]==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(norm_ped[:,0],'-*', label='normal')\n",
    "plt.plot(abnormal_ped[:,0], '-+', label='abnormal')\n",
    "plt.xlabel('pedestrian')\n",
    "plt.ylabel('Averaged IOU over time')\n",
    "plt.legend()\n",
    "# sns.distplot(norm_ped[:,0])\n",
    "# sns.distplot(abnormal_ped[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding distribution of normal vs abnormal pedestrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_abnorm = []\n",
    "iou_norm = []\n",
    "for j in ped.keys():\n",
    "#     print(j)\n",
    "    x,y = norm_train_max_min(data_dict=ped[j], max1 = max1, min1=min1 )\n",
    "    out1 = lstm_20.predict(x)\n",
    "    out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "#     print(out.shape)\n",
    "    if np.any(ped[j]['abnormal']):\n",
    "#         iou_abnorm.append(np.squeeze(out).tolist()[:])\n",
    "        iou_abnorm += np.squeeze(out).tolist()\n",
    "    else:\n",
    "#         iou_norm.append(np.squeeze(out).tolist())\n",
    "        iou_norm += np.squeeze(out).tolist()\n",
    "\n",
    "# iou_abnorm = np.array(iou_abnorm)\n",
    "# iou_norm = np.array(iou_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(iou_abnorm, label='abnormal pedestrain')\n",
    "sns.distplot(iou_norm, label='normal pedestrain')\n",
    "plt.title('Test Video 1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testdict['abnormal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding distrubtions of Normal vs abnormal frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startvid =0\n",
    "endvid=21\n",
    "\n",
    "testdict = Boxes(loc_files_test[startvid:endvid], box_test_txt[startvid:endvid], frames, pad ='pre', to_xywh = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out1 = lstm_20.predict(x)\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "out = np.squeeze(out)\n",
    "#     print(out.shape)\n",
    "norm_index = np.where(testdict['abnormal']== 0)\n",
    "abnorm_index = np.where(testdict['abnormal']== 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abnorm_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(out[abnorm_index], label='abnormal frames')\n",
    "sns.distplot(out[norm_index], label='normal frames')\n",
    "plt.title('All Test Video ')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification on frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out1 = lstm_20.predict(x)\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "out = np.squeeze(out)\n",
    "np.random.seed(49)\n",
    "rand = np.random.permutation(len(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # create model\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(1, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, out[rand], testdict['abnormal'][rand], cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline: %.2f%% (%.8f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "startvid =0\n",
    "endvid=21\n",
    "\n",
    "testdict = Boxes(loc_files_test[startvid:endvid], box_test_txt[startvid:endvid], frames, pad ='pre', to_xywh = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_abnormal_normal_frames(testdict, out, abnormal_ratio, val_ratio, weight_scale):\n",
    "\n",
    "    # Finds Abnormal index\n",
    "    abnorm_index = np.where(testdict['abnormal']==1)\n",
    "    norm_index = np.where(testdict['abnormal'] == 0)\n",
    "    rand_an = np.random.permutation(len(abnorm_index[0]))\n",
    "    rand_n = np.random.permutation(len(norm_index[0]))\n",
    "    abnorm_index = abnorm_index[0][rand_an]\n",
    "    norm_index = norm_index[0][rand_n]\n",
    "\n",
    "    \n",
    "    \n",
    "    len_abn_split = int(len(abnorm_index)*abnormal_ratio)\n",
    "    \n",
    "    # Index for the training set before being split\n",
    "    # Weight scale allows for a much higher factor of \n",
    "    # normal values to exist\n",
    "    abn = abnorm_index[:len_abn_split]\n",
    "    norm = norm_index[:len_abn_split*weight_scale]\n",
    "    \n",
    "\n",
    "    \n",
    "    len_val_split = int(val_ratio*len(abn) )\n",
    "    \n",
    "    # NOte how len_val_split is used in  val_a , val_b, \n",
    "    val_abn= abn[:len_val_split]\n",
    "    val_norm = norm[:len_val_split*weight_scale]\n",
    "    \n",
    "    train_abn = abn[len_val_split:]\n",
    "    train_norm = norm[len_val_split*weight_scale:]\n",
    "    \n",
    "    \n",
    "    test_abn = abnorm_index[len_abn_split:]\n",
    "    test_norm = norm_index[len_abn_split*weight_scale:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_x_an = out[ train_abn]\n",
    "    train_y_an= testdict['abnormal'][ train_abn ]\n",
    "    \n",
    "    train_x_n = out[ train_norm]\n",
    "    train_y_n = testdict['abnormal'][ train_norm ]\n",
    "    \n",
    "    train_x = np.append(train_x_an,train_x_n,axis =0)\n",
    "    train_y = np.append(train_y_an,train_y_n, axis = 0 )\n",
    "    \n",
    "    \n",
    "    val_x_an = out[ val_abn]\n",
    "    val_y_an= testdict['abnormal'][ val_abn ]\n",
    "    \n",
    "    val_x_n = out[ val_norm]\n",
    "    val_y_n = testdict['abnormal'][ val_norm ]\n",
    "    \n",
    "    val_x = np.append(val_x_an,val_x_n,axis =0)\n",
    "    val_y = np.append(val_y_an,val_y_n, axis = 0 )\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_x_an = out[ test_abn ]\n",
    "    test_y_an= testdict['abnormal'][ test_abn]\n",
    "    \n",
    "    test_x_n = out[ test_norm] \n",
    "    test_y_n = testdict['abnormal'][ test_norm ]\n",
    "    \n",
    "    test_x = np.append(test_x_an, test_x_n, axis = 0)\n",
    "    test_y = np.append(test_y_an, test_y_n, axis = 0)\n",
    "    \n",
    "    return train_x, train_y,val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out1 = lstm_20.predict(x)\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "out = np.squeeze(out)\n",
    "np.random.seed(49)\n",
    "rand = np.random.permutation(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    " abnormal_ratio, val_ratio = 0.5,0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y,val_x, val_y, test_x, test_y = vary_abnormal_normal_frames(testdict,\n",
    "                                                                            out,\n",
    "                                                                            abnormal_ratio,\n",
    "                                                                            val_ratio,\n",
    "                                                                            weight_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "train_univariate_1 = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "train_univariate_1 = train_univariate_1.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_univariate_1 = tf.data.Dataset.from_tensor_slices((val_x,val_y))\n",
    "val_univariate_1 = val_univariate_1.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_univariate_1 = tf.data.Dataset.from_tensor_slices((test_x,test_y))\n",
    "# train_univariate_1 = train_univariate_1.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifcation Even Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # create model\n",
    "        neurons = 20\n",
    "        dropout_ratio = 0.3\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(dropout_ratio))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(dropout_ratio))\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(49)\n",
    "# rand = np.random.permutation(len(out))\n",
    "# size = 115624\n",
    "# test ={}\n",
    "\n",
    "# for i in testdict.keys():\n",
    "#     testdict[i] = testdict[i][rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = create_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,321\n",
      "Trainable params: 1,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "binary_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train, y_test =train_test_split(out[rand], testdict['abnormal'][rand],shuffle=False,test_size=.5)\n",
    "# Bad thing is can't guantree how many samples of abnormal is in x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train, y_test =train_test_split(out, testdict['abnormal'],shuffle=False,test_size=.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, \n",
    "                                                  patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0536 - accuracy: 0.9864 - val_loss: 8.2355 - val_accuracy: 0.5000\n",
      "Epoch 2/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0522 - accuracy: 0.9870 - val_loss: 6.5365 - val_accuracy: 0.5000\n",
      "Epoch 3/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0490 - accuracy: 0.9870 - val_loss: 6.5668 - val_accuracy: 0.5000\n",
      "Epoch 4/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0488 - accuracy: 0.9870 - val_loss: 6.1632 - val_accuracy: 0.5000\n",
      "Epoch 5/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0485 - accuracy: 0.9870 - val_loss: 5.8312 - val_accuracy: 0.5000\n",
      "Epoch 6/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0474 - accuracy: 0.9870 - val_loss: 5.0944 - val_accuracy: 0.5000\n",
      "Epoch 7/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0480 - accuracy: 0.9870 - val_loss: 5.0342 - val_accuracy: 0.5000\n",
      "Epoch 8/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0465 - accuracy: 0.9870 - val_loss: 6.0938 - val_accuracy: 0.5000\n",
      "Epoch 9/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0480 - accuracy: 0.9870 - val_loss: 5.4096 - val_accuracy: 0.5000\n",
      "Epoch 10/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0474 - accuracy: 0.9870 - val_loss: 5.4729 - val_accuracy: 0.5000\n",
      "Epoch 11/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0468 - accuracy: 0.9870 - val_loss: 5.6948 - val_accuracy: 0.5000\n",
      "Epoch 12/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0478 - accuracy: 0.9870 - val_loss: 5.3782 - val_accuracy: 0.5000\n",
      "Epoch 13/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0478 - accuracy: 0.9870 - val_loss: 5.3174 - val_accuracy: 0.5000\n",
      "Epoch 14/300\n",
      "3522/3522 [==============================] - 15s 4ms/step - loss: 0.0467 - accuracy: 0.9870 - val_loss: 5.2631 - val_accuracy: 0.5000\n",
      "Epoch 15/300\n",
      "2787/3522 [======================>.......] - ETA: 3s - loss: 0.0594 - accuracy: 0.9836"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fe91bd989b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_univariate_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_univariate_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.virtualenvs/anomalous_pred/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bm = binary_model.fit(train_univariate_1, validation_data=val_univariate_1, epochs=300,callbacks=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bm.history['val_loss'], label='val_loss')\n",
    "plt.plot(bm.history['loss'], label='loss')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bm.history['val_accuracy'], label='val_acc')\n",
    "plt.plot(bm.history['accuracy'], label='acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = binary_model.predict(test_x)\n",
    "plt.plot(y_pred, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = binary_model.predict(test_x) > 0.5\n",
    "tf.math.confusion_matrix(test_y,y_pred , num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their does not seem to be a need to create a custom loss.\n",
    "Reason is that weihted_cross_entropy_with_logits exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startvid =0\n",
    "endvid=21\n",
    "\n",
    "testdict = Boxes(loc_files_test[startvid:endvid], box_test_txt[startvid:endvid], frames, pad ='pre', to_xywh = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale = 10\n",
    "# initial_bias = np.log([ratio/(ratio*weight_scale)])\n",
    "initial_bias = np.log([1/weight_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out1 = lstm_20.predict(x)\n",
    "out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "out = np.squeeze(out)\n",
    "np.random.seed(49)\n",
    "rand = np.random.permutation(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vary_abnormal_normal_frames():\n",
    "\n",
    "    # Finds Abnormal index\n",
    "    abnorm_index = np.where(testdict['abnormal']==1)\n",
    "    norm_index = np.where(testdict['abnormal'] == 0)\n",
    "#     rand_an = np.random.permutation(len(abnorm_index[0]))\n",
    "#     rand_n = np.random.permutation(len(abnormabnorm_index[0]))\n",
    "#     abnorm_index = abnorm_index[0][rand_an]\n",
    "#     norm_index = norm_index[0][rand_n]\n",
    "    \n",
    "    abnormal_ratio = 0.5\n",
    "    val_ratio = 0.2\n",
    "    \n",
    "    \n",
    "    len_abn_split = int(len(abnorm_index[0])*abnormal_ratio)\n",
    "    \n",
    "    # Index for the training set before being split\n",
    "    # Weight scale allows for a much higher factor of \n",
    "    # normal values to exist\n",
    "    abn = abnorm_index[0][:len_abn_split]\n",
    "    norm = norm_index[0][:len_abn_split*weight_scale]\n",
    "    \n",
    "\n",
    "    \n",
    "    len_val_split = int(val_ratio*len(abn) )\n",
    "    \n",
    "    # NOte how len_val_split is used in  val_a , val_b, \n",
    "    val_abn= abn[:len_val_split]\n",
    "    val_norm = norm[:len_val_split*weight_scale]\n",
    "    \n",
    "    train_abn = abn[len_val_split:]\n",
    "    train_norm = norm[len_val_split*weight_scale:]\n",
    "    \n",
    "    \n",
    "    test_abn = abnorm_index[0][len_abn_split:]\n",
    "    test_norm = norm_index[0][len_abn_split*weight_scale:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_x_an = out[ train_abn]\n",
    "    train_y_an= testdict['abnormal'][ train_abn ]\n",
    "    \n",
    "    train_x_n = out[ train_norm]\n",
    "    train_y_n = testdict['abnormal'][ train_norm ]\n",
    "    \n",
    "    train_x = np.append(train_x_an,train_x_n,axis =0)\n",
    "    train_y = np.append(train_y_an,train_y_n, axis = 0 )\n",
    "    \n",
    "    \n",
    "    val_x_an = out[ val_abn]\n",
    "    val_y_an= testdict['abnormal'][ val_abn ]\n",
    "    \n",
    "    val_x_n = out[ val_norm]\n",
    "    val_y_n = testdict['abnormal'][ val_norm ]\n",
    "    \n",
    "    val_x = np.append(val_x_an,val_x_n,axis =0)\n",
    "    val_y = np.append(val_y_an,val_y_n, axis = 0 )\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_x_an = out[ test_abn ]\n",
    "    test_y_an= testdict['abnormal'][ test_abn]\n",
    "    \n",
    "    test_x_n = out[ test_norm] \n",
    "    test_y_n = testdict['abnormal'][ test_norm ]\n",
    "    \n",
    "    test_x = np.append(test_x_an, test_x_n, axis = 0)\n",
    "    test_y = np.append(test_y_an, test_y_n, axis = 0)\n",
    "    \n",
    "#     return \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((val_x,val_y))\n",
    "val_univariate = val_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias = np.log([1/weight_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training')\n",
    "print('abnormal_mean: {}, normal_mean: {}'.format(train_x_an.mean(), train_x_n.mean()))\n",
    "print('abnormal_mean: {}, normal_mean: {}'.format(train_y_an.mean(), train_y_n.mean()))\n",
    "print(\"\\n\")\n",
    "\n",
    "print('testing')\n",
    "print('abnormal_mean: {}, normal_mean: {}'.format(test_x_an.mean(), test_x_n.mean()))\n",
    "print('abnormal_mean: {}, normal_mean: {}'.format(test_y_an.mean(), test_y_n.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary(y,x):\n",
    "    return tf.nn.weighted_cross_entropy_with_logits(y,x,pos_weight = weight_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([.8], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant([1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.nn.weighted_cross_entropy_with_logits(y,x,pos_weight = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.losses.binary_crossentropy(y, x, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_weighted(output_bias = None):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # create model\n",
    "        if output_bias is not None:\n",
    "            output_bias = keras.initializers.Constant(output_bias)\n",
    "        neurons = 30\n",
    "        dropout_ratio = 0.3\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(neurons, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(dropout_ratio))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(neurons, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(dropout_ratio))\n",
    "        model.add(keras.layers.Dense(1, bias_initializer=output_bias))\n",
    "        # Compile model\n",
    "#         wce = tf.nn.weighted_cross_entropy_with_logits(pos_weight = 10)\n",
    "        model.compile(loss=weighted_binary, optimizer='adam', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wl = create_baseline_weighted( )\n",
    "model_wl.layers[-1].bias.assign([0.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0005, \n",
    "                                                  patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wl.predict(test_x).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbm = model_wl.fit(train_univariate, validation_data=val_univariate, epochs=300,callbacks = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wbm.history['loss'], '-*',label='loss')\n",
    "plt.plot(wbm.history['val_loss'], '-o',label='val_loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wbm.history['accuracy'],'-*', label='acc')\n",
    "plt.plot(wbm.history['val_accuracy'],'-*', label='val_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbm.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_wl.predict(test_x)\n",
    "plt.plot(y_pred, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_wl.predict(test_x) >0.5\n",
    "tf.math.confusion_matrix(test_y, y_pred, num_classes =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XY difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out = lstm_20.predict(x)\n",
    "# np.random.seed(49)\n",
    "# rand = np.random.permutation(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_diff = []\n",
    "for i in range(1,len(out)):\n",
    "    \n",
    "    xy_diff.append(out[i,:] - out[i-1,:])\n",
    "\n",
    "xy_diff = np.array(xy_diff)\n",
    "out =np.sum(np.square(xy_diff[:,0:2]), axis=1)\n",
    "# out =np.sum(xy_diff[:,0:4], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)  XY difference from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_index = np.where(testdict['abnormal'][1:]== 0)\n",
    "abnorm_index = np.where(testdict['abnormal'][1:]== 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(out[abnorm_index], label='abnormal frames')\n",
    "sns.distplot(out[norm_index], label='normal frames')\n",
    "plt.title('Test Video 1')\n",
    "plt.legend()\n",
    "plt.xlabel('XY Difference')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) XY difference  predicted frame from ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out = lstm_20.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_diff = []\n",
    "for i in range(0,len(out)):\n",
    "    \n",
    "    xy_diff.append(out[i,:] - y[i,:])\n",
    "\n",
    "xy_diff = np.array(xy_diff)\n",
    "out =np.sum(np.square(xy_diff[:,0:4]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_index = np.where(testdict['abnormal'][1:]== 0)\n",
    "abnorm_index = np.where(testdict['abnormal'][1:]== 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(out[abnorm_index], label='abnormal frames')\n",
    "sns.distplot(out[norm_index], label='normal frames')\n",
    "plt.title('Difference from GT Test Video 2')\n",
    "plt.legend()\n",
    "plt.xlabel('XY Difference')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # create model\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(1, input_dim=1, activation='relu'))\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=32, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, out[rand], testdict['abnormal'][rand], cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ped[j]['frame_ppl_id'][1:,-1,0], out,'*')\n",
    "# plt.plot(ped['1']['frame_ppl_id'][1:,-1,0], xy_diff[:,0],'-+')\n",
    "# plt.plot(ped['1']['frame_ppl_id'][1:,-1,0], xy_diff[:,1],'-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped[j]['abnormal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abnormal Through IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j ='1'\n",
    "out1 = lstm_20.predict(ped[j]['x_ppl_box'])\n",
    "out = norm_train_max_min(data=out1, max1 = max1,min1 =min1,undo_norm=True)\n",
    "bb_intersection_over_union(xywh_tlbr(out), xywh_tlbr(ped[j]['y_ppl_box']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means Clustering Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = norm_train_max_min(data_dict=testdict, max1 = max1, min1=min1 )\n",
    "out = lstm_20.predict(x)\n",
    "# out = bb_intersection_over_union_np(xywh_tlbr(out1),xywh_tlbr(y))\n",
    "# out = np.squeeze(out)\n",
    "# np.random.seed(49)\n",
    "# rand = np.random.permutation(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_scal = model.predict(xx_test)\n",
    "# y_pred = norm_train_max_min(data=y_pred_scal, max1 = max1,min1 =min1,undo_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_scal = KMeans(n_clusters=2)\n",
    "kmean = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_scal.fit(y_pred_scal[:,0:2])\n",
    "kmean.fit(y_pred[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = kmean_scal.cluster_centers_\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = kmean.cluster_centers_\n",
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindict['frame_ppl_id'][0:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped = pedsort(testdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j='86'\n",
    "x,y = norm_train_max_min(data_dict=ped[j], max1=max1, min1=min1 )\n",
    "\n",
    "out1 = model.predict(x)\n",
    "bb_intersection_over_union(out1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(y_pred_scal[:,0], y_pred_scal[:,1],s=1, c='b')\n",
    "plt.scatter(out[0,0], out[0,1],s=200, c ='g', marker='s')\n",
    "plt.scatter(out[1,0], out[1,1], s=200, c ='r', marker='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred[:,0], y_pred[:,1],s=1, c='b')\n",
    "plt.scatter(out1[0,0], out1[0,1],s=200, c ='g', marker='s')\n",
    "plt.scatter(out1[1,0], out1[1,1], s=200, c ='r', marker='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestion that x,y is not enough to create a hyoer parameter to seperate abnomaly and normal pedestrains. Makes sense because abnormal and normal pedestrains walk over the same spatial features. Trying to do it for all pedestrains "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
